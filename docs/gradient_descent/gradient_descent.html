<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Principles and Techniques of Data Science - 12&nbsp; Gradient Descent</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../feature_engineering/feature_engineering.html" rel="next">
<link href="../ols/ols.html" rel="prev">
<link href="../data100_logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
        <script type="text/javascript">
        window.PlotlyConfig = {MathJaxConfig: 'local'};
        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
        if (typeof require !== 'undefined') {
        require.undef("plotly");
        requirejs.config({
            paths: {
                'plotly': ['https://cdn.plot.ly/plotly-2.3.1.min']
            }
        });
        require(['plotly'], function(Plotly) {
            window._Plotly = Plotly;
        });
        }
        </script>
        

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Gradient Descent</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../data100_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Principles and Techniques of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/DS-100/course-notes-su23" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Welcome</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_lec/introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_1/pandas_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Pandas I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_2/pandas_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_3/pandas_3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas III</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../eda/eda.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Cleaning and EDA</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../regex/regex.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Text Wrangling and Regular Expressions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_1/visualization_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visualization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sampling/sampling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Sampling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_to_modeling/intro_to_modeling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Introduction to Modeling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../constant_model_loss_transformations/loss_transformations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ols/ols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../gradient_descent/gradient_descent.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Gradient Descent</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../feature_engineering/feature_engineering.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Sklearn and Feature Engineering</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_HCE/case_study_HCE.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../cv_regularization/cv_reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Gradient Descent</h2>
   
  <ul>
  <li><a href="#minimizing-a-1d-function" id="toc-minimizing-a-1d-function" class="nav-link active" data-scroll-target="#minimizing-a-1d-function"><span class="toc-section-number">12.1</span>  Minimizing a 1D Function</a>
  <ul>
  <li><a href="#the-naive-approach-guess-and-check" id="toc-the-naive-approach-guess-and-check" class="nav-link" data-scroll-target="#the-naive-approach-guess-and-check"><span class="toc-section-number">12.1.1</span>  The Naive Approach: Guess and Check</a></li>
  <li><a href="#scipy.optimize.minimize" id="toc-scipy.optimize.minimize" class="nav-link" data-scroll-target="#scipy.optimize.minimize"><span class="toc-section-number">12.1.2</span>  <code>scipy.optimize.minimize</code></a></li>
  <li><a href="#finding-the-minimum-algorithmically" id="toc-finding-the-minimum-algorithmically" class="nav-link" data-scroll-target="#finding-the-minimum-algorithmically"><span class="toc-section-number">12.1.3</span>  Finding the Minimum Algorithmically</a></li>
  </ul></li>
  <li><a href="#gradient-descent-on-a-1d-model" id="toc-gradient-descent-on-a-1d-model" class="nav-link" data-scroll-target="#gradient-descent-on-a-1d-model"><span class="toc-section-number">12.2</span>  Gradient Descent on a 1D Model</a>
  <ul>
  <li><a href="#convexity" id="toc-convexity" class="nav-link" data-scroll-target="#convexity"><span class="toc-section-number">12.2.1</span>  Convexity</a></li>
  </ul></li>
  <li><a href="#multi-dimensional-gradient-descent" id="toc-multi-dimensional-gradient-descent" class="nav-link" data-scroll-target="#multi-dimensional-gradient-descent"><span class="toc-section-number">12.3</span>  Multi-Dimensional Gradient Descent</a>
  <ul>
  <li><a href="#loss-surfaces" id="toc-loss-surfaces" class="nav-link" data-scroll-target="#loss-surfaces"><span class="toc-section-number">12.3.1</span>  Loss Surfaces</a></li>
  <li><a href="#partial-derivatives" id="toc-partial-derivatives" class="nav-link" data-scroll-target="#partial-derivatives"><span class="toc-section-number">12.3.2</span>  Partial Derivatives</a></li>
  <li><a href="#gradient-descent-in-vector-form" id="toc-gradient-descent-in-vector-form" class="nav-link" data-scroll-target="#gradient-descent-in-vector-form"><span class="toc-section-number">12.3.3</span>  Gradient Descent in Vector Form</a></li>
  </ul></li>
  <li><a href="#batch-mini-batch-and-stochastic-gradient-descent" id="toc-batch-mini-batch-and-stochastic-gradient-descent" class="nav-link" data-scroll-target="#batch-mini-batch-and-stochastic-gradient-descent"><span class="toc-section-number">12.4</span>  Batch, Mini-Batch, and Stochastic Gradient Descent</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title d-none d-lg-block"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Gradient Descent</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Learning Outcomes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Describe the conceptual basis for gradient descent</li>
<li>Compute the gradient descent update on a provided dataset</li>
</ul>
</div>
</div>
</div>
<p>At this point, we’re fairly comfortable with fitting a regression model under MSE risk (indeed, we’ve done it twice now!). We have seen how we can algebraically solve for the minimizing value of the model parameter <span class="math inline">\(\theta\)</span>, as well as how we can use linear algebra to determine the optimal parameters geometrically.</p>
<p>It’s important to remember, however, that the results we’ve found previously apply to one very specific case: the derivations we performed previously are only relevant to a linear regression model using MSE as the cost function. In reality, we’ll be working with a wide range of model types and objective functions, not all of which are as straightforward as the scenario we’ve discussed previously. This means that we need some more generalizable way of fitting a model to minimize loss.</p>
<p>To do this, we’ll introduce the technique of <strong>gradient descent</strong>.</p>
<section id="minimizing-a-1d-function" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="minimizing-a-1d-function"><span class="header-section-number">12.1</span> Minimizing a 1D Function</h2>
<p>Let’s shift our focus away from MSE to consider some new, arbitrary cost function. You can think of this function as outputting the empirical risk associated with some parameter <code>theta</code>.</p>
<p><img src="images/arbitrary.png" alt="arbitrary" width="600"></p>
<p>Our goal is to find the input that <em>minimzes</em> this arbitrary function. In other words, we want to find the position along the x-axis where the function is at its minimum. In a modeling context, you can imagine attempting to find the value of <span class="math inline">\(\theta\)</span> that results in the lowest possible loss for a model.</p>
<section id="the-naive-approach-guess-and-check" class="level3" data-number="12.1.1">
<h3 data-number="12.1.1" class="anchored" data-anchor-id="the-naive-approach-guess-and-check"><span class="header-section-number">12.1.1</span> The Naive Approach: Guess and Check</h3>
<p>Above, we saw that the minimum is somewhere around 5.3ish. Let’s see if we can figure out how to find the exact minimum algorithmically from scratch. One way very slow and terrible way would be to manually guess-and-check. The code below “guesses” several possible minimizing values and checks to see which one gives the lowest value of the function.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> arbitrary(theta):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The function we would like to minimize</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (theta<span class="op">**</span><span class="dv">4</span> <span class="op">-</span> <span class="dv">15</span><span class="op">*</span>theta<span class="op">**</span><span class="dv">3</span> <span class="op">+</span> <span class="dv">80</span><span class="op">*</span>theta<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">180</span><span class="op">*</span>theta <span class="op">+</span> <span class="dv">144</span>)<span class="op">/</span><span class="dv">10</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_minimize(f, thetas):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> [f(theta) <span class="cf">for</span> theta <span class="kw">in</span> thetas]  </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> thetas[np.argmin(y)]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>guesses <span class="op">=</span> [<span class="fl">5.3</span>, <span class="fl">5.31</span>, <span class="fl">5.32</span>, <span class="fl">5.33</span>, <span class="fl">5.34</span>, <span class="fl">5.35</span>]</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>simple_minimize(arbitrary, guesses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>5.33</code></pre>
</div>
</div>
<p>This technique moves slowly: we have to manually specify what values we would like to guess. It is also imprecise – if the true minimizing value happened to lie <em>between</em> two of our guesses, we would have no way of identifying it. We want a more rigorous method of identifying the minimizing value.</p>
</section>
<section id="scipy.optimize.minimize" class="level3" data-number="12.1.2">
<h3 data-number="12.1.2" class="anchored" data-anchor-id="scipy.optimize.minimize"><span class="header-section-number">12.1.2</span> <code>scipy.optimize.minimize</code></h3>
<p>Another method to minimize this mathematical function is to use the <code>optimize.minimize</code> function from the <code>scipy</code> library. It takes a function and a starting guess, then, it locates the minimum of the function.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># In the readout below, `x` corresponds to the minimizing value of the function</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>minimize(arbitrary, x0 <span class="op">=</span> <span class="fl">3.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>      fun: -0.13827491292966557
 hess_inv: array([[0.73848255]])
      jac: array([6.48573041e-06])
  message: 'Optimization terminated successfully.'
     nfev: 20
      nit: 3
     njev: 10
   status: 0
  success: True
        x: array([2.39275266])</code></pre>
</div>
</div>
<p><code>scipy.optimize.minimize</code> is great. It may also seem a bit magical. How can this one line of code find the minimum of any mathematical function so quickly?</p>
<p>Behind the scenes, <code>scipy.optimize.minimize</code> uses a technique called <strong>gradient descent</strong> to compute the minimizing value of a function. In this lecture, we will learn the underlying theory behind gradient descent, then implement it ourselves.</p>
</section>
<section id="finding-the-minimum-algorithmically" class="level3" data-number="12.1.3">
<h3 data-number="12.1.3" class="anchored" data-anchor-id="finding-the-minimum-algorithmically"><span class="header-section-number">12.1.3</span> Finding the Minimum Algorithmically</h3>
<p>Looking at the function across this domain, it is clear that the function’s minimum value occurs around <span class="math inline">\(\theta = 5.3\)</span>. Let’s pretend for a moment that we <em>couldn’t</em> see the full view of the cost function. How would we guess the value of <span class="math inline">\(\theta\)</span> that minimizes the function?</p>
<p>It turns out that the first derivative of the function can give us a clue. In the plots below, the line indicates the value of the function’s derivative at each value of <span class="math inline">\(\theta\)</span>. The derivative is negative where it is red and positive where it is green.</p>
<p>Say we make a guess for the minimizing value of <span class="math inline">\(\theta\)</span>. Remember that we read plots from left to right, and assume that our starting <span class="math inline">\(\theta\)</span> value is to the left of the optimal <span class="math inline">\(\hat{\theta}\)</span>. If the guess “undershoots” the true minimizing value – our guess for <span class="math inline">\(\theta\)</span> is not quite at the value of the <span class="math inline">\(\hat{\theta}\)</span> that truly minimizes the function – the derivative will be <strong>negative</strong> in value. This means that if we increase <span class="math inline">\(\theta\)</span> (move further to the right), then we <strong>can decrease</strong> our loss function further. If this guess “overshoots” the true minimizing value, the derivative will be positive in value, implying the converse.</p>
<p><img src="images/step.png" alt="step" width="600"></p>
<p>We can use this pattern to help formulate our next guess for the optimal <span class="math inline">\(\hat{\theta}\)</span>. Consider the case where we’ve undershot <span class="math inline">\(\theta\)</span> by guessing too low of a value. We’ll want our next guess to be greater in value than the previous guess – that is, we want to shift our guess to the right. You can think of this as following the slope “downhill” to the function’s minimum value.</p>
<p><img src="images/neg_step.png" alt="neg_step" width="600"></p>
<p>If we’ve overshot <span class="math inline">\(\hat{\theta}\)</span> by guessing too high of a value, we’ll want our next guess to be lower in value – we want to shift our guess for <span class="math inline">\(\hat{\theta}\)</span> to the left. Again, we follow the slope of the curve downhill towards the minimum value.</p>
<p><img src="images/pos_step.png" alt="pos_step" width="600"></p>
</section>
</section>
<section id="gradient-descent-on-a-1d-model" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="gradient-descent-on-a-1d-model"><span class="header-section-number">12.2</span> Gradient Descent on a 1D Model</h2>
<p>These observations lead us to the <strong>gradient descent update rule</strong>: <span class="math display">\[\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{d}{d\theta}L(\theta^{(t)})\]</span></p>
<p>Begin with our guess for <span class="math inline">\(\hat{\theta}\)</span> at timestep <span class="math inline">\(t\)</span>. To find our guess for <span class="math inline">\(\hat{\theta}\)</span> at the next timestep, <span class="math inline">\(t+1\)</span>, subtract the objective function’s derivative <span class="math inline">\(\frac{d}{d\theta} L(\theta^{(t)})\)</span> scaled by a positive value <span class="math inline">\(\alpha\)</span>. We’ve replaced the generic function <span class="math inline">\(f\)</span> with <span class="math inline">\(L\)</span> to indicate that we are minimizing loss.</p>
<ul>
<li>If our guess <span class="math inline">\(\theta^{(t)}\)</span> is to the left of <span class="math inline">\(\hat{\theta}\)</span> (undershooting), the first derivative will be negative. Subtracting a negative number from <span class="math inline">\(\theta^{(t)}\)</span> will <em>increase</em> the value of the next guess, <span class="math inline">\(\theta^{(t+1)}\)</span>. The guess will shift to the right.</li>
<li>If our guess <span class="math inline">\(\theta^{(t)}\)</span> was too high (overshooting <span class="math inline">\(\hat{\theta}\)</span>), the first derivative will be positive. Subtracting a positive number from <span class="math inline">\(\theta^{(t)}\)</span> will <em>decrease</em> the value of the next guess, <span class="math inline">\(\theta^{(t+1)}\)</span>. The guess will shift to the left.</li>
</ul>
<p>Put together, this captures the same behavior we reasoned through above. We repeatedly update our guess for the optimal <span class="math inline">\(\theta\)</span> until we’ve completed a set number of updates, or until each additional update iteration does not change the value of <span class="math inline">\(\theta\)</span>. In this second case, we say that gradient descent has <strong>converged</strong> on a solution. Our choice of which stopping condition to use will depending on the specific application.</p>
<p>The <span class="math inline">\(\alpha\)</span> term in the update rule is known as the <strong>learning rate</strong>. It is a positive value represents the size of each gradient descent update step – in other words, how “far” should we step to the left or right with each updated guess? A high value of <span class="math inline">\(\alpha\)</span> will lead to large differences in value between consecutive guesses for <span class="math inline">\(\hat{\theta}\)</span>; a low value of <span class="math inline">\(\alpha\)</span> will result in smaller differences in value between consecutive guesses. This is our first example of a <strong>hyperparameter</strong> – a parameter hand-picked by the data scientist that changes the model’s behavior – in this course.</p>
<p>If we run gradient descent on our arbitrary function, we produce a more precise estimate of the minimizing <span class="math inline">\(\theta\)</span>.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the derivative of the arbitrary function we want to minimize</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> derivative_arbitrary(theta):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="dv">4</span><span class="op">*</span>theta<span class="op">**</span><span class="dv">3</span> <span class="op">-</span> <span class="dv">45</span><span class="op">*</span>theta<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">160</span><span class="op">*</span>theta <span class="op">-</span> <span class="dv">180</span>)<span class="op">/</span><span class="dv">10</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(df, initial_guess, alpha, n):</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Performs n steps of gradient descent on df using learning rate alpha starting</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">       from initial_guess. Returns a numpy array of all guesses over time."""</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    guesses <span class="op">=</span> [initial_guess]</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    current_guess <span class="op">=</span> initial_guess</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(guesses) <span class="op">&lt;</span> n:</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        current_guess <span class="op">=</span> current_guess <span class="op">-</span> alpha <span class="op">*</span> df(current_guess)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        guesses.append(current_guess)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(guesses)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>trajectory <span class="op">=</span> gradient_descent(derivative_arbitrary, <span class="dv">4</span>, <span class="fl">0.3</span>, <span class="dv">20</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">1</span>, <span class="dv">7</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, arbitrary(thetas))</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>plt.scatter(trajectory, arbitrary(trajectory), c<span class="op">=</span><span class="st">"white"</span>, edgecolor<span class="op">=</span><span class="st">"firebrick"</span>, label<span class="op">=</span><span class="st">"Previous guesses"</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>plt.scatter(trajectory[<span class="op">-</span><span class="dv">1</span>], arbitrary(trajectory[<span class="op">-</span><span class="dv">1</span>]), c<span class="op">=</span><span class="st">"firebrick"</span>, label<span class="op">=</span><span class="st">"Final guess"</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta$"</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Cost"</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Minimizing theta: </span><span class="sc">{</span>trajectory[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Minimizing theta: 5.326344077237774</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="gradient_descent_files/figure-html/cell-4-output-2.png" width="587" height="429"></p>
</div>
</div>
<section id="convexity" class="level3" data-number="12.2.1">
<h3 data-number="12.2.1" class="anchored" data-anchor-id="convexity"><span class="header-section-number">12.2.1</span> Convexity</h3>
<p>In our analysis above, we focused our attention on the global minimum of the loss function. You may be wondering: what about the local minimum just to the left?</p>
<p>If we had chosen a different starting guess for <span class="math inline">\(\theta\)</span>, or a different value for the learning rate <span class="math inline">\(\alpha\)</span>, we may have converged on the local minimum, rather than on the true optimum value of loss.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>local_min_trajectory <span class="op">=</span> gradient_descent(derivative_arbitrary, <span class="fl">1.6</span>, <span class="fl">0.75</span>, <span class="dv">20</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, arbitrary(thetas))</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>plt.scatter(local_min_trajectory, arbitrary(local_min_trajectory), c<span class="op">=</span><span class="st">"white"</span>, edgecolor<span class="op">=</span><span class="st">"firebrick"</span>, label<span class="op">=</span><span class="st">"Previous guesses"</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>plt.scatter(local_min_trajectory[<span class="op">-</span><span class="dv">1</span>], arbitrary(local_min_trajectory[<span class="op">-</span><span class="dv">1</span>]), c<span class="op">=</span><span class="st">"firebrick"</span>, label<span class="op">=</span><span class="st">"Final guess"</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta$"</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Cost"</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Minimizing theta: </span><span class="sc">{</span>local_min_trajectory[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Minimizing theta: 2.39274798112695</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="gradient_descent_files/figure-html/cell-5-output-2.png" width="587" height="429"></p>
</div>
</div>
<p>The arbitrary function we have been working with is non-convex: it contains both local and global minima.</p>
<p>Conversely, if a loss function is <strong>convex</strong>, gradient descent is guaranteed to find the global minimum of the objective function. Formally, a function <span class="math inline">\(f\)</span> is convex if:</p>
<p><span class="math display">\[tf(a) + (1-t)f(b) \geq f(ta + (1-t)b)\]</span></p>
<p>To put this into words: if you drew a line between any two points on the curve, all values on the curve must be <em>on or below</em> the line. Importantly, any local minimum of a convex function is also its global minimum.</p>
<p><img src="images/convex.png" alt="convex" width="600"></p>
<p>Why does this matter? Non-convex loss functions can cause problems with optimization. This means that our choice of loss function is an key factor in our modeling process. It turns out that MSE <em>is</em> convex, which is a major reason why it is such a popular choice of loss function.</p>
</section>
</section>
<section id="multi-dimensional-gradient-descent" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="multi-dimensional-gradient-descent"><span class="header-section-number">12.3</span> Multi-Dimensional Gradient Descent</h2>
<p>We’re in good shape now: we’ve developed a technique to find the minimum value of a more complex objective function.</p>
<p>The function we worked with above was one-dimensional – we were only minimizing the function with respect to a single parameter, <span class="math inline">\(\theta\)</span>. However, as we’ve seen before, we often need to optimize a cost function with respect to several parameters (for example, when selecting the best model parameters for multiple linear regression). We’ll need to extend our gradient descent rule to <em>multi-dimensional</em> objective functions.</p>
<p>Let’s consider our familiar <code>tips</code> dataset.</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>tips <span class="op">=</span> sns.load_dataset(<span class="st">"tips"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>tips.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="5">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>total_bill</th>
      <th>tip</th>
      <th>sex</th>
      <th>smoker</th>
      <th>day</th>
      <th>time</th>
      <th>size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>16.99</td>
      <td>1.01</td>
      <td>Female</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10.34</td>
      <td>1.66</td>
      <td>Male</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>21.01</td>
      <td>3.50</td>
      <td>Male</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>23.68</td>
      <td>3.31</td>
      <td>Male</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>24.59</td>
      <td>3.61</td>
      <td>Female</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="loss-surfaces" class="level3" data-number="12.3.1">
<h3 data-number="12.3.1" class="anchored" data-anchor-id="loss-surfaces"><span class="header-section-number">12.3.1</span> Loss Surfaces</h3>
<p>Suppose we want to apply simple linear regression to predict the <code>"tip"</code> from the <code>"total_bill"</code> and an intercept term. Our model takes the form:</p>
<p><span class="math display">\[\hat{y} = \theta_0 + \theta_1 x\]</span></p>
<p>We’ll use the MSE loss function to quantify our model’s error.</p>
<p><span class="math display">\[\text{MSE}(\theta_0,\:\theta_1) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - \theta_0 - \theta_1 x)^2\]</span></p>
<p>Notice that our model contains two parameters, <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span>. This means that we now need to optimize <em>two</em> different parameters to determine their optimal values. Rather than a one-dimensional loss function like we saw in the previous section, we are now dealing with a <strong>loss surface</strong>.</p>
<p>A loss surface visualizes how the model’s loss changes with different <em>combinations</em> of parameter values. Each point on the surface tells us what the model’s loss will be for a given choice of <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span>. Click and drag on the visualization below to explore the surface. We have marked the optimal choice of model parameters in red.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This code is for illustration purposes only</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># It contains a lot of syntax you have not seen before</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># For now, just focus on the outputted plot</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.linear_model <span class="im">as</span> lm</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> lm.LinearRegression(fit_intercept <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>tips[<span class="st">"bias"</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>model.fit(tips[[<span class="st">"bias"</span>,<span class="st">"total_bill"</span>]], tips[<span class="st">"tip"</span>])</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>uvalues <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">10</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>vvalues <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="dv">10</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>(u,v) <span class="op">=</span> np.meshgrid(uvalues, vvalues)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.vstack((u.flatten(),v.flatten()))</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> tips[[<span class="st">"bias"</span>,<span class="st">"total_bill"</span>]].to_numpy()</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> tips[<span class="st">"tip"</span>].to_numpy()</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_loss_single_arg(theta):</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mse_loss(theta, X, Y)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_loss(theta, X, y_obs):</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> X <span class="op">@</span> theta</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean((y_hat <span class="op">-</span> Y) <span class="op">**</span> <span class="dv">2</span>)    </span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>MSE <span class="op">=</span> np.array([mse_loss_single_arg(t) <span class="cf">for</span> t <span class="kw">in</span> thetas.T])</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>loss_surface <span class="op">=</span> go.Surface(x<span class="op">=</span>u, y<span class="op">=</span>v, z<span class="op">=</span>np.reshape(MSE, u.shape))</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>ind <span class="op">=</span> np.argmin(MSE)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>optimal_point <span class="op">=</span> go.Scatter3d(name <span class="op">=</span> <span class="st">"Optimal Point"</span>,</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> [thetas.T[ind,<span class="dv">0</span>]], y <span class="op">=</span> [thetas.T[ind,<span class="dv">1</span>]], </span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> [MSE[ind]],</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">"red"</span>))</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> go.Figure(data<span class="op">=</span>[loss_surface, optimal_point])</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>fig.update_layout(scene <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>    xaxis_title <span class="op">=</span> <span class="st">"theta0"</span>,</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>    yaxis_title <span class="op">=</span> <span class="st">"theta1"</span>,</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>    zaxis_title <span class="op">=</span> <span class="st">"MSE"</span>))</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">

<div>                            <div id="653cf025-2590-4e0a-944a-f4afcdb31318" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("653cf025-2590-4e0a-944a-f4afcdb31318")) {                    Plotly.newPlot(                        "653cf025-2590-4e0a-944a-f4afcdb31318",                        [{"type":"surface","x":[[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0]],"y":[[-0.2,-0.2,-0.2,-0.2,-0.2,-0.2,-0.2,-0.2,-0.2,-0.2],[-0.12222222222222223,-0.12222222222222223,-0.12222222222222223,-0.12222222222222223,-0.12222222222222223,-0.12222222222222223,-0.12222222222222223,-0.12222222222222223,-0.12222222222222223,-0.12222222222222223],[-0.04444444444444445,-0.04444444444444445,-0.04444444444444445,-0.04444444444444445,-0.04444444444444445,-0.04444444444444445,-0.04444444444444445,-0.04444444444444445,-0.04444444444444445,-0.04444444444444445],[0.033333333333333326,0.033333333333333326,0.033333333333333326,0.033333333333333326,0.033333333333333326,0.033333333333333326,0.033333333333333326,0.033333333333333326,0.033333333333333326,0.033333333333333326],[0.1111111111111111,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.1111111111111111],[0.18888888888888888,0.18888888888888888,0.18888888888888888,0.18888888888888888,0.18888888888888888,0.18888888888888888,0.18888888888888888,0.18888888888888888,0.18888888888888888,0.18888888888888888],[0.26666666666666666,0.26666666666666666,0.26666666666666666,0.26666666666666666,0.26666666666666666,0.26666666666666666,0.26666666666666666,0.26666666666666666,0.26666666666666666,0.26666666666666666],[0.3444444444444445,0.3444444444444445,0.3444444444444445,0.3444444444444445,0.3444444444444445,0.3444444444444445,0.3444444444444445,0.3444444444444445,0.3444444444444445,0.3444444444444445],[0.4222222222222222,0.4222222222222222,0.4222222222222222,0.4222222222222222,0.4222222222222222,0.4222222222222222,0.4222222222222222,0.4222222222222222,0.4222222222222222,0.4222222222222222],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5]],"z":[[71.6689479180328,64.79495237057277,58.31601855150779,52.232146460837896,46.54333609856305,41.24958746468326,36.35090055919854,31.84727538210889,27.738711933414294,24.025210213114757],[46.284202926684884,40.778124400070844,35.66710760185185,30.951152532027933,26.63025919059907,22.70442757756527,19.17365769292654,16.037949536682863,13.297303108834248,10.951718409380694],[26.5908582295082,22.452696723740136,18.709596946367135,15.361558897389196,12.408582576806314,9.8506679846185,7.6878151208257455,5.9200239854280525,4.547294578425421,3.5696268998178513],[12.588913826502734,9.818669341580652,7.443486585053635,5.463365556921677,3.8783062571847813,2.688308685842947,1.8933728428961754,1.493498728344465,1.4886863421878163,1.8789356844262293],[4.278369717668489,2.876042253592391,1.8687765179113542,1.2565725106253798,1.0394302317344666,1.2173496812386155,1.7903308591378257,2.7583737654320974,4.121478400121431,5.879644763205828],[1.6592259030054644,1.624815459775349,1.9854667449402952,2.7411797585003033,3.8919545004553724,5.437790970805505,7.378689169550697,9.714649096690952,12.445670752226269,15.571754136156649],[4.731482382513661,6.064988960129529,7.793557266140458,9.917187300546447,12.435879063347496,15.349632554543616,18.65844777413479,22.36232472212102,26.46126339850232,30.955263803278687],[13.495139156193087,16.196562754654938,19.293048081511852,22.784595136763826,26.671203920410857,30.95287443245296,35.62960667289012,40.70140064172232,46.16825633894962,52.03017376457197],[27.950196224043715,32.01953684335154,36.48393919105444,41.3434032671524,46.59792907164542,52.247516604533494,58.292165865816635,64.73187685549483,71.5666495735681,78.79648402003644],[48.09665358606557,53.53391122621939,59.36623059476827,65.5936116917122,72.2160545170512,79.23355907078526,86.6461253529144,94.45375336343858,102.65644310235781,111.25419456967214]]},{"marker":{"color":"red","size":10},"name":"Optimal Point","type":"scatter3d","x":[0.7777777777777777],"y":[0.1111111111111111],"z":[1.0394302317344666]}],                        {"scene":{"xaxis":{"title":{"text":"theta0"}},"yaxis":{"title":{"text":"theta1"}},"zaxis":{"title":{"text":"MSE"}}},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('653cf025-2590-4e0a-944a-f4afcdb31318');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<p>It is often easier to visualize a loss surface by using a contour plot. You can imagine the visualization below as being a “bird’s eye view” of the 2D loss surface from above.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>loss_contour <span class="op">=</span> go.Contour(x<span class="op">=</span>u[<span class="dv">0</span>], y<span class="op">=</span>v[:, <span class="dv">0</span>], z<span class="op">=</span>np.reshape(MSE, u.shape))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> go.Figure(data<span class="op">=</span>[loss_contour])</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>fig.update_layout(scene <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    xaxis_title <span class="op">=</span> <span class="st">"theta0"</span>,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    yaxis_title <span class="op">=</span> <span class="st">"theta1"</span>,</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    zaxis_title <span class="op">=</span> <span class="st">"MSE"</span>))</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">

<div>                            <div id="5bd18115-92c9-4d55-96bc-f14559f79539" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("5bd18115-92c9-4d55-96bc-f14559f79539")) {                    Plotly.newPlot(                        "5bd18115-92c9-4d55-96bc-f14559f79539",                        [{"type":"contour","x":[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],"y":[-0.2,-0.12222222222222223,-0.04444444444444445,0.033333333333333326,0.1111111111111111,0.18888888888888888,0.26666666666666666,0.3444444444444445,0.4222222222222222,0.5],"z":[[71.6689479180328,64.79495237057277,58.31601855150779,52.232146460837896,46.54333609856305,41.24958746468326,36.35090055919854,31.84727538210889,27.738711933414294,24.025210213114757],[46.284202926684884,40.778124400070844,35.66710760185185,30.951152532027933,26.63025919059907,22.70442757756527,19.17365769292654,16.037949536682863,13.297303108834248,10.951718409380694],[26.5908582295082,22.452696723740136,18.709596946367135,15.361558897389196,12.408582576806314,9.8506679846185,7.6878151208257455,5.9200239854280525,4.547294578425421,3.5696268998178513],[12.588913826502734,9.818669341580652,7.443486585053635,5.463365556921677,3.8783062571847813,2.688308685842947,1.8933728428961754,1.493498728344465,1.4886863421878163,1.8789356844262293],[4.278369717668489,2.876042253592391,1.8687765179113542,1.2565725106253798,1.0394302317344666,1.2173496812386155,1.7903308591378257,2.7583737654320974,4.121478400121431,5.879644763205828],[1.6592259030054644,1.624815459775349,1.9854667449402952,2.7411797585003033,3.8919545004553724,5.437790970805505,7.378689169550697,9.714649096690952,12.445670752226269,15.571754136156649],[4.731482382513661,6.064988960129529,7.793557266140458,9.917187300546447,12.435879063347496,15.349632554543616,18.65844777413479,22.36232472212102,26.46126339850232,30.955263803278687],[13.495139156193087,16.196562754654938,19.293048081511852,22.784595136763826,26.671203920410857,30.95287443245296,35.62960667289012,40.70140064172232,46.16825633894962,52.03017376457197],[27.950196224043715,32.01953684335154,36.48393919105444,41.3434032671524,46.59792907164542,52.247516604533494,58.292165865816635,64.73187685549483,71.5666495735681,78.79648402003644],[48.09665358606557,53.53391122621939,59.36623059476827,65.5936116917122,72.2160545170512,79.23355907078526,86.6461253529144,94.45375336343858,102.65644310235781,111.25419456967214]]}],                        {"scene":{"xaxis":{"title":{"text":"theta0"}},"yaxis":{"title":{"text":"theta1"}},"zaxis":{"title":{"text":"MSE"}}},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('5bd18115-92c9-4d55-96bc-f14559f79539');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
</section>
<section id="partial-derivatives" class="level3" data-number="12.3.2">
<h3 data-number="12.3.2" class="anchored" data-anchor-id="partial-derivatives"><span class="header-section-number">12.3.2</span> Partial Derivatives</h3>
<p>Though our objective function looks a little different, we can use the same principles as we did earlier to locate the optimal model parameters. Notice how the minimum value of MSE, marked by the red dot in the plot above, occurs in the “valley” of the loss surface. Like before, we want our guesses for the best pair of <span class="math inline">\((\theta_0,\:\theta_1)\)</span> to move “downhill” towards this minimum point.</p>
<p>With a small adjustment, we can use the same gradient descent algorithm we derived earlier. Our loss function is now a <strong>multivariable function</strong>. It depends on two variables, <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span>. We’ll need to take the derivative with respect to one of these variables at a time.</p>
<p>A <strong>partial derivative</strong> is the derivative of a multivariable function with respect to just <em>one</em> variable. We treat all other variables in the function as constants, and instead focus our attention on just the variable of interest. For a multivariable function <span class="math inline">\(f(x, y)\)</span>, the symbol <span class="math inline">\(\frac{\partial f} {\partial x}\)</span> denotes the partial derivative of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(x\)</span> (holding <span class="math inline">\(y\)</span> constant), while the symbol <span class="math inline">\(\frac{\partial f} {\partial y}\)</span> denotes the partial derivative of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(y\)</span> (holding <span class="math inline">\(x\)</span> constant).</p>
<p><span class="math display">\[f(x, y) = 3x^2 + y\]</span> <span class="math display">\[\frac{\partial f} {\partial x} = 6x\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\frac{\partial f} {\partial y} = 1\]</span></p>
<p>We can interpret a partial derivative as saying “if change one variable while holding all other variables constant, how will the function change?”</p>
</section>
<section id="gradient-descent-in-vector-form" class="level3" data-number="12.3.3">
<h3 data-number="12.3.3" class="anchored" data-anchor-id="gradient-descent-in-vector-form"><span class="header-section-number">12.3.3</span> Gradient Descent in Vector Form</h3>
<p>Recall our gradient descent update rule from before:</p>
<p><span class="math display">\[\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{d}{d\theta}L(\theta^{(t)})\]</span></p>
<p>When optimizing a model with just one parameter, we iteratively updated guesses for only one <span class="math inline">\(\theta\)</span>.</p>
<p>In our new model with two parameters, we need to update guesses for <em>both</em> <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span> that minimize our loss function <span class="math inline">\(L(\theta_0, \theta_1)\)</span>. This means that we apply our gradient update rule for <em>each</em> parameter <span class="math inline">\(\theta_i\)</span>. Because the loss function <span class="math inline">\(L(\theta_0, \theta_1)\)</span> is now a function of two variables, we compute its partial derivatives with respect to <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span>.</p>
<p><span class="math display">\[\theta_0^{(t+1)} = \theta_0^{(t)} - \alpha \frac{\partial L}{\partial \theta_0}\Bigm\vert_{\theta=\theta^{(t)}} \qquad \qquad \theta_1^{(t+1)} = \theta_1^{(t)} - \alpha \frac{\partial L}{\partial \theta_1}\Bigm\vert_{\theta=\theta^{(t)}}\]</span></p>
<p>We can tidy this statement up by using vector notation: <span class="math display">\[\begin{bmatrix}
           \theta_{0}^{(t+1)} \\
           \theta_{1}^{(t+1)} \\
         \end{bmatrix}
=
\begin{bmatrix}
           \theta_{0}^{(t)} \\
           \theta_{1}^{(t)} \\
         \end{bmatrix}
- \alpha
\begin{bmatrix}
           \frac{\partial L}{\partial \theta_{0}}\vert_{\theta=\theta^{(t)}} \\
           \frac{\partial L}{\partial \theta_{1}}\vert_{\theta=\theta^{(t)}} \\
         \end{bmatrix}
\]</span></p>
<p>To save ourselves from writing out long column vectors, we’ll introduce some new notation. <span class="math inline">\(\vec{\theta}^{(t)}\)</span> is a column vector of guesses for each model parameter <span class="math inline">\(\theta_i\)</span> at timestep <span class="math inline">\(t\)</span>. We call <span class="math inline">\(\nabla_{\vec{\theta}} L\)</span> the <strong>gradient vector.</strong> In plain English, it means “take the derivative of loss, <span class="math inline">\(L\)</span>, with respect to each model parameter in <span class="math inline">\(\vec{\theta}\)</span>, and evaluate it at the given <span class="math inline">\(\theta = \theta^{(t)}\)</span>.”</p>
<p><span class="math display">\[\vec{\theta}^{(t+1)}
= \vec{\theta}^{(t)} - \alpha \nabla_{\vec{\theta}} L(\theta^{(t)})
\]</span></p>
<p>Let’s see this in action. In the cell below, we compute the gradient vector for our choice of model and loss function, then run the gradient descent algorithm. We see that our final guess lies very close to the true minimum of the loss surface!</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_gradient(theta, X, y_obs):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the gradient of the MSE on our data for the given theta"""</span>    </span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    x0 <span class="op">=</span> X.iloc[:, <span class="dv">0</span>]</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    x1 <span class="op">=</span> X.iloc[:, <span class="dv">1</span>]</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    dth0 <span class="op">=</span> np.mean(<span class="op">-</span><span class="dv">2</span> <span class="op">*</span> (y_obs <span class="op">-</span> theta[<span class="dv">0</span>]<span class="op">*</span>x0 <span class="op">-</span> theta[<span class="dv">1</span>]<span class="op">*</span>x1) <span class="op">*</span> x0)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    dth1 <span class="op">=</span> np.mean(<span class="op">-</span><span class="dv">2</span> <span class="op">*</span> (y_obs <span class="op">-</span> theta[<span class="dv">0</span>]<span class="op">*</span>x0 <span class="op">-</span> theta[<span class="dv">1</span>]<span class="op">*</span>x1) <span class="op">*</span> x1)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array([dth0, dth1])</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_gradient_single_arg(theta):</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the gradient of the MSE on our data for the given theta"""</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> tips[[<span class="st">"bias"</span>, <span class="st">"total_bill"</span>]]</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    y_obs <span class="op">=</span> tips[<span class="st">"tip"</span>]</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mse_gradient(theta, X, y_obs)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_single_arg(theta):</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    theta_0, theta_1 <span class="op">=</span> theta[<span class="dv">0</span>], theta[<span class="dv">1</span>]</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean((tips[<span class="st">"tip"</span>]<span class="op">-</span>theta_0<span class="op">-</span>theta_1<span class="op">*</span>tips[<span class="st">"total_bill"</span>])<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>guesses <span class="op">=</span> gradient_descent(mse_gradient_single_arg, np.array([<span class="fl">0.5</span>, <span class="fl">0.3</span>]), <span class="fl">0.002</span>, <span class="dv">1000</span>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>mses <span class="op">=</span> [mse_single_arg(theta) <span class="cf">for</span> theta <span class="kw">in</span> guesses]</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>trajectory <span class="op">=</span> go.Scatter3d(x<span class="op">=</span>guesses[:, <span class="dv">0</span>], y<span class="op">=</span>guesses[:, <span class="dv">1</span>], z<span class="op">=</span>mses, marker<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">3</span>))</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> go.Figure(data<span class="op">=</span>[loss_surface, trajectory])</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>fig.update_layout(scene <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    xaxis_title <span class="op">=</span> <span class="st">"theta0"</span>,</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>    yaxis_title <span class="op">=</span> <span class="st">"theta1"</span>,</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>    zaxis_title <span class="op">=</span> <span class="st">"MSE"</span>))</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">

<div>                            <div id="e21e8d65-01ec-4f62-b10c-4588ffecce71" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("e21e8d65-01ec-4f62-b10c-4588ffecce71")) {                    Plotly.newPlot(                        "e21e8d65-01ec-4f62-b10c-4588ffecce71",                        [{"type":"surface","x":[[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0],[-1.0,-0.5555555555555556,-0.11111111111111116,0.33333333333333326,0.7777777777777777,1.2222222222222223,1.6666666666666665,2.1111111111111107,2.5555555555555554,3.0]],"y":[[-0.2,-0.2,-0.2,-0.2,-0.2,-0.2,-0.2,-0.2,-0.2,-0.2],[-0.12222222222222223,-0.12222222222222223,-0.12222222222222223,-0.12222222222222223,-0.12222222222222223,-0.12222222222222223,-0.12222222222222223,-0.12222222222222223,-0.12222222222222223,-0.12222222222222223],[-0.04444444444444445,-0.04444444444444445,-0.04444444444444445,-0.04444444444444445,-0.04444444444444445,-0.04444444444444445,-0.04444444444444445,-0.04444444444444445,-0.04444444444444445,-0.04444444444444445],[0.033333333333333326,0.033333333333333326,0.033333333333333326,0.033333333333333326,0.033333333333333326,0.033333333333333326,0.033333333333333326,0.033333333333333326,0.033333333333333326,0.033333333333333326],[0.1111111111111111,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.1111111111111111,0.1111111111111111],[0.18888888888888888,0.18888888888888888,0.18888888888888888,0.18888888888888888,0.18888888888888888,0.18888888888888888,0.18888888888888888,0.18888888888888888,0.18888888888888888,0.18888888888888888],[0.26666666666666666,0.26666666666666666,0.26666666666666666,0.26666666666666666,0.26666666666666666,0.26666666666666666,0.26666666666666666,0.26666666666666666,0.26666666666666666,0.26666666666666666],[0.3444444444444445,0.3444444444444445,0.3444444444444445,0.3444444444444445,0.3444444444444445,0.3444444444444445,0.3444444444444445,0.3444444444444445,0.3444444444444445,0.3444444444444445],[0.4222222222222222,0.4222222222222222,0.4222222222222222,0.4222222222222222,0.4222222222222222,0.4222222222222222,0.4222222222222222,0.4222222222222222,0.4222222222222222,0.4222222222222222],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5]],"z":[[71.6689479180328,64.79495237057277,58.31601855150779,52.232146460837896,46.54333609856305,41.24958746468326,36.35090055919854,31.84727538210889,27.738711933414294,24.025210213114757],[46.284202926684884,40.778124400070844,35.66710760185185,30.951152532027933,26.63025919059907,22.70442757756527,19.17365769292654,16.037949536682863,13.297303108834248,10.951718409380694],[26.5908582295082,22.452696723740136,18.709596946367135,15.361558897389196,12.408582576806314,9.8506679846185,7.6878151208257455,5.9200239854280525,4.547294578425421,3.5696268998178513],[12.588913826502734,9.818669341580652,7.443486585053635,5.463365556921677,3.8783062571847813,2.688308685842947,1.8933728428961754,1.493498728344465,1.4886863421878163,1.8789356844262293],[4.278369717668489,2.876042253592391,1.8687765179113542,1.2565725106253798,1.0394302317344666,1.2173496812386155,1.7903308591378257,2.7583737654320974,4.121478400121431,5.879644763205828],[1.6592259030054644,1.624815459775349,1.9854667449402952,2.7411797585003033,3.8919545004553724,5.437790970805505,7.378689169550697,9.714649096690952,12.445670752226269,15.571754136156649],[4.731482382513661,6.064988960129529,7.793557266140458,9.917187300546447,12.435879063347496,15.349632554543616,18.65844777413479,22.36232472212102,26.46126339850232,30.955263803278687],[13.495139156193087,16.196562754654938,19.293048081511852,22.784595136763826,26.671203920410857,30.95287443245296,35.62960667289012,40.70140064172232,46.16825633894962,52.03017376457197],[27.950196224043715,32.01953684335154,36.48393919105444,41.3434032671524,46.59792907164542,52.247516604533494,58.292165865816635,64.73187685549483,71.5666495735681,78.79648402003644],[48.09665358606557,53.53391122621939,59.36623059476827,65.5936116917122,72.2160545170512,79.23355907078526,86.6461253529144,94.45375336343858,102.65644310235781,111.25419456967214]]},{"marker":{"size":3},"type":"scatter3d","x":[0.5,0.48624998360655736,0.49895837678365956,0.4882513329554657,0.4982660431554448,0.4899421231682547,0.4978471020882492,0.4913895232802911,0.4976421124882162,0.49264613920729583,0.49760452128006316,0.49375317126660523,0.49769786907442537,0.4947428870963906,0.497893601692037,0.49564055840571,0.49816935618511826,0.4964659778056013,0.4985076184772452,0.49723464676648726,0.4988946720487617,0.49795870600706466,0.4993197745643197,0.49864766415972767,0.4997745130209805,0.49930896844945866,0.5002522987107129,0.4999484516402401,0.5007479716832474,0.5005706820762041,0.5012574909678286,0.5011792378281601,0.5017776919599366,0.5017769214006891,0.5023060964105204,0.5023659278872439,0.5028407636126602,0.5029479766664838,0.5033801738533865,0.5035244145447008,0.5039231371350436,0.504096296535286,0.5044687216873475,0.5046644491238941,0.5050161979791966,0.5052295198166891,0.5055649948696326,0.505792015945724,0.5061146652659879,0.5063523350606819,0.5066648592279073,0.5069107887311941,0.5072153029028551,0.5074676211884269,0.507765782028749,0.5080230239248705,0.508316129013491,0.5085771471286549,0.5088662128158675,0.5091301086387214,0.5094159310204355,0.509682000958367,0.5099652036307023,0.510232896748138,0.5105139682080461,0.5107828531277758,0.5110621760645976,0.5113319150454314,0.5116097892815679,0.5118801179163788,0.5121567783740509,0.5124274896896132,0.5127031204621361,0.5129740524663778,0.5132487978385533,0.5135198237677677,0.5137937968468763,0.5140648175274966,0.5143381070029507,0.5146090448694162,0.5148817203068101,0.5151525147164578,0.5154246307037815,0.5156952342675447,0.5159668336624313,0.516237209371103,0.5165083258440204,0.5167784448175897,0.517049104843627,0.5173189445685967,0.5175891689873988,0.5178587119362807,0.5181285171737642,0.5183977497238912,0.518667148749072,0.518936060335829,0.5192050634101953,0.5194736458638437,0.5197422611282523,0.5200105081545422,0.5202787420888656,0.52054664886226,0.5208145066453761,0.5210820694904704,0.5213495552822,0.5216167714242155,0.5218838885861311,0.5221507559555058,0.5224175072238663,0.5226840243032133,0.5229504119244035,0.5232165776286525,0.523482603465257,0.5237484170477819,0.5240140826616613,0.5242795436407623,0.5245448503581167,0.5248099584594418,0.5250749074217679,0.5253396625332185,0.5256042547372187,0.525868656873633,0.5261328932024716,0.5263969424779651,0.5266608237257476,0.5269245203320508,0.5271880472229964,0.5274513914124895,0.5277145646159463,0.5279775566883725,0.5282403768305779,0.5285030171226376,0.5287654847959284,0.5290277736731297,0.5292898894431562,0.5295518272934325,0.5298135917048084,0.5300751789335202,0.5303365925142468,0.530597829540267,0.5308588928052,0.5311197800578485,0.531380493511412,0.531641031428055,0.5319013955663677,0.5321615845905375,0.5324215999030785,0.5326814404830016,0.532941107453914,0.5332006000413579,0.5334599191504715,0.5337190641998424,0.5339780359234736,0.5342368338911092,0.5344954587026883,0.5347539100463047,0.5350121884168672,0.5352702935951253,0.5355282259936966,0.5357859854658639,0.5360435723597615,0.5363009865854454,0.5365582284405148,0.5368152978794561,0.5370721951602565,0.5373289202721662,0.5375854734421156,0.5378418546865481,0.5380980642080376,0.5383541020442913,0.5386099683787744,0.5388656632658142,0.5391211868738769,0.5393765392702735,0.5396317206116898,0.5398867309755723,0.5401415705093475,0.5403962392983667,0.5406507374827716,0.5409050651540711,0.5411592224466689,0.5414132094568633,0.5416670263145307,0.5419206731196876,0.5421741499986319,0.5424274570542592,0.5426805944100317,0.5429335621710668,0.5431863604585732,0.5434389893793755,0.5436914490528845,0.5439437395872292,0.5441958611003795,0.5444478137014532,0.5446995975072589,0.5449512126276561,0.5452026591785112,0.545453937270232,0.5457050470179141,0.5459559885323622,0.5462067619280357,0.5464573673160175,0.5467078048102362,0.5469580745219597,0.5472081765646687,0.5474581110497435,0.5477078780902814,0.5479574777977184,0.5482069102848189,0.5484561756630301,0.5487052740448234,0.5489542055416223,0.5492029702656365,0.5494515683282385,0.5496999998414006,0.5499482649164233,0.5501963636650608,0.5504442961985248,0.5506920626283661,0.5509396630656951,0.5511870976218712,0.551434366407893,0.551681469534938,0.5519284071138851,0.5521751792557373,0.5524217860712475,0.55266822767125,0.5529145041663672,0.5531606156672692,0.5534065622844441,0.5536523441284014,0.5538979613094921,0.5541434139380682,0.5543887021243414,0.5546338259785082,0.5548787856106393,0.5551235811307779,0.5553682126488524,0.5556126802747539,0.5558569841182677,0.556101124289134,0.5563451008969946,0.5565889140514396,0.5568325638619663,0.557076050438016,0.5573193738889414,0.5575625343240351,0.5578055318525051,0.5580483665834964,0.5582910386260718,0.5585335480892288,0.5587758950818853,0.5590180797128921,0.5592601020910215,0.5595019623249782,0.5597436605233893,0.5599851967948134,0.5602265712477326,0.5604677839905595,0.5607088351316315,0.5609497247792155,0.561190453041504,0.5614310200266187,0.5616714258426075,0.5619116705974472,0.5621517543990407,0.5623916773552203,0.5626314395737446,0.5628710411623011,0.5631104822285045,0.5633497628798977,0.5635888832239512,0.563827843368064,0.5640666434195628,0.5643052834857026,0.5645437636736663,0.5647820840905652,0.5650202448434387,0.5652582460392547,0.5654960877849091,0.5657337701872266,0.5659712933529599,0.5662086573887906,0.5664458624013284,0.566682908497112,0.5669197957826084,0.5671565243642135,0.5673930943482517,0.5676295058409763,0.5678657589485694,0.568101853777142,0.5683377904327339,0.5685735690213141,0.5688091896487801,0.5690446524209591,0.569279957443607,0.5695151048224089,0.5697500946629791,0.5699849270708612,0.5702196021515281,0.570454120010382,0.5706884807527544,0.5709226844839064,0.5711567313090284,0.5713906213332406,0.5716243546615924,0.5718579313990632,0.5720913516505617,0.5723246155209266,0.5725577231149264,0.5727906745372591,0.5730234698925528,0.5732561092853655,0.573488592820185,0.5737209206014292,0.5739530927334461,0.5741851093205136,0.5744169704668399,0.5746486762765635,0.5748802268537527,0.5751116223024066,0.5753428627264544,0.5755739482297555,0.5758048789160999,0.576035654889208,0.576266276252731,0.5764967431102501,0.5767270555652776,0.5769572137212562,0.5771872176815595,0.5774170675494914,0.5776467634282872,0.5778763054211127,0.5781056936310645,0.5783349281611703,0.5785640091143887,0.5787929365936093,0.579021710701653,0.5792503315412715,0.5794787992151478,0.5797071138258961,0.5799352754760619,0.580163284268122,0.5803911403044844,0.5806188436874885,0.5808463945194055,0.5810737929024375,0.5813010389387187,0.5815281327303143,0.5817550743792218,0.5819818639873698,0.5822085016566189,0.5824349874887612,0.582661321585521,0.582887504048554,0.5831135349794482,0.5833394144797233,0.5835651426508309,0.583790719594155,0.5840161454110113,0.5842414202026479,0.5844665440702449,0.5846915171149146,0.5849163394377017,0.5851410111395831,0.5853655323214679,0.5855899030841981,0.5858141235285476,0.586038193755223,0.5862621138648635,0.5864858839580407,0.586709504135259,0.5869329744969555,0.5871562951434997,0.5873794661751941,0.587602487692274,0.5878253597949075,0.5880480825831956,0.5882706561571723,0.5884930806168045,0.588715356061992,0.588937482592568,0.5891594603082985,0.5893812893088828,0.5896029696939535,0.5898245015630763,0.5900458850157503,0.5902671201514078,0.5904882070694146,0.5907091458690699,0.5909299366496064,0.5911505795101902,0.5913710745499212,0.5915914218678326,0.5918116215628916,0.5920316737339987,0.5922515784799884,0.5924713358996292,0.5926909460916229,0.5929104091546055,0.5931297251871468,0.5933488942877507,0.5935679165548551,0.5937867920868318,0.5940055209819868,0.5942241033385601,0.5944425392547262,0.5946608288285935,0.5948789721582048,0.5950969693415373,0.5953148204765024,0.5955325256609459,0.5957500849926483,0.5959674985693242,0.5961847664886232,0.5964018888481291,0.5966188657453604,0.5968356972777704,0.597052383542747,0.5972689246376129,0.5974853206596257,0.5977015717059775,0.5979176778737956,0.5981336392601422,0.5983494559620144,0.5985651280763443,0.5987806556999992,0.5989960389297811,0.5992112778624278,0.5994263725946116,0.5996413232229406,0.5998561298439579,0.6000707925541419,0.6002853114499064,0.6004996866276007,0.6007139181835095,0.6009280062138529,0.6011419508147867,0.6013557520824021,0.6015694101127261,0.6017829250017213,0.6019962968452858,0.6022095257392538,0.6024226117793952,0.6026355550614155,0.6028483556809564,0.6030610137335953,0.6032735293148458,0.6034859025201572,0.6036981334449151,0.603910222184441,0.6041221688339927,0.6043339734887643,0.6045456362438857,0.6047571571944235,0.6049685364353803,0.6051797740616952,0.6053908701682438,0.6056018248498379,0.605812638201226,0.6060233103170929,0.6062338412920601,0.6064442312206859,0.6066544801974648,0.6068645883168283,0.6070745556731446,0.6072843823607187,0.6074940684737923,0.6077036141065439,0.6079130193530893,0.6081222843074808,0.608331409063708,0.6085403937156972,0.6087492383573122,0.6089579430823536,0.6091665079845593,0.6093749331576043,0.6095832186951008,0.6097913646905985,0.6099993712375842,0.6102072384294823,0.6104149663596544,0.6106225551213996,0.6108300048079547,0.6110373155124936,0.6112444873281282,0.6114515203479078,0.6116584146648195,0.6118651703717879,0.6120717875616756,0.6122782663272827,0.6124846067613473,0.6126908089565455,0.612896873005491,0.6131027990007357,0.6133085870347694,0.61351423720002,0.6137197495888533,0.6139251242935735,0.6141303614064226,0.6143354610195811,0.6145404232251677,0.6147452481152392,0.614949935781791,0.6151544863167566,0.6153588998120081,0.615563176359356,0.6157673160505492,0.6159713189772753,0.6161751852311603,0.616378914903769,0.6165825080866045,0.6167859648711091,0.6169892853486636,0.6171924696105874,0.6173955177481389,0.6175984298525155,0.6178012060148531,0.618003846326227,0.6182063508776512,0.6184087197600788,0.618610953064402,0.6188130508814519,0.6190150133019992,0.6192168404167532,0.6194185323163629,0.6196200890914164,0.6198215108324412,0.6200227976299039,0.6202239495742107,0.6204249667557071,0.6206258492646783,0.6208265971913489,0.621027210625883,0.6212276896583842,0.6214280343788959,0.6216282448774012,0.6218283212438228,0.6220282635680232,0.6222280719398047,0.6224277464489094,0.6226272871850194,0.6228266942377565,0.6230259676966826,0.6232251076512997,0.6234241141910496,0.6236229874053143,0.623821727383416,0.624020334214617,0.6242188079881197,0.6244171487930668,0.6246153567185412,0.6248134318535663,0.6250113742871058,0.6252091841080638,0.6254068614052847,0.6256044062675534,0.6258018187835956,0.6259990990420773,0.626196247131605,0.6263932631407262,0.6265901471579287,0.6267868992716413,0.6269835195702335,0.6271800081420154,0.6273763650752381,0.6275725904580935,0.6277686843787147,0.6279646469251753,0.6281604781854901,0.628356178247615,0.628551747199447,0.628747185128824,0.6289424921235253,0.6291376682712712,0.6293327136597233,0.6295276283764846,0.6297224125090992,0.6299170661450527,0.630111589371772,0.6303059822766255,0.6305002449469229,0.6306943774699156,0.6308883799327966,0.6310822524227001,0.6312759950267024,0.6314696078318212,0.6316630909250158,0.6318564443931876,0.6320496683231794,0.6322427628017762,0.6324357279157043,0.6326285637516327,0.6328212703961715,0.6330138479358733,0.6332062964572326,0.6333986160466859,0.6335908067906117,0.633782868775331,0.6339748020871064,0.6341666068121433,0.6343582830365889,0.6345498308465329,0.6347412503280073,0.6349325415669864,0.6351237046493869,0.6353147396610681,0.6355056466878316,0.6356964258154215,0.6358870771295246,0.6360776007157702,0.6362679966597303,0.6364582650469195,0.636648405962795,0.6368384194927571,0.6370283057221485,0.6372180647362551,0.6374076966203053,0.6375972014594705,0.6377865793388652,0.6379758303435467,0.6381649545585154,0.6383539520687148,0.6385428229590314,0.6387315673142949,0.6389201852192781,0.639108676758697,0.6392970420172108,0.6394852810794223,0.6396733940298771,0.6398613809530647,0.6400492419334175,0.6402369770553117,0.6404245864030669,0.640612070060946,0.6407994281131557,0.6409866606438461,0.6411737677371111,0.6413607494769882,0.6415476059474584,0.6417343372324468,0.6419209434158218,0.6421074245813962,0.6422937808129262,0.642480012194112,0.6426661188085977,0.6428521007399716,0.6430379580717656,0.6432236908874559,0.6434092992704629,0.6435947833041508,0.643780143071828,0.6439653786567473,0.6441504901421056,0.6443354776110439,0.6445203411466479,0.6447050808319472,0.644889696749916,0.645074188983473,0.6452585576154811,0.6454428027287479,0.6456269244060253,0.6458109227300101,0.6459947977833432,0.6461785496486107,0.6463621784083429,0.6465456841450151,0.6467290669410474,0.6469123268788043,0.6470954640405954,0.6472784785086751,0.6474613703652429,0.6476441396924427,0.6478267865723639,0.6480093110870407,0.6481917133184524,0.6483739933485232,0.6485561512591226,0.6487381871320651,0.6489201010491106,0.649101893091964,0.6492835633422755,0.6494651118816409,0.649646538791601,0.6498278441536419,0.6500090280491955,0.6501900905596387,0.6503710317662942,0.6505518517504301,0.6507325505932601,0.6509131283759434,0.6510935851795849,0.6512739210852353,0.6514541361738907,0.651634230526493,0.6518142042239302,0.6519940573470357,0.652173789976589,0.6523534021933154,0.652532894077886,0.6527122657109179,0.6528915171729744,0.6530706485445648,0.653249659906144,0.6534285513381137,0.6536073229208212,0.6537859747345602,0.6539645068595705,0.6541429193760382,0.6543212123640957,0.6544993859038216,0.6546774400752411,0.6548553749583256,0.6550331906329928,0.6552108871791072,0.6553884646764796,0.6555659232048672,0.6557432628439739,0.6559204836734503,0.6560975857728936,0.6562745692218475,0.6564514340998024,0.6566281804861959,0.6568048084604118,0.656981318101781,0.6571577094895812,0.6573339827030371,0.6575101378213202,0.6576861749235489,0.6578620940887887,0.6580378953960523,0.658213578924299,0.6583891447524357,0.6585645929593161,0.6587399236237412,0.6589151368244592,0.6590902326401655,0.6592652111495029,0.6594400724310613,0.6596148165633781,0.6597894436249381,0.6599639536941735,0.6601383468494638,0.6603126231691363,0.6604867827314655,0.6606608256146737,0.6608347518969305,0.6610085616563536,0.6611822549710079,0.6613558319189062,0.6615292925780092,0.661702637026225,0.6618758653414097,0.6620489776013674,0.6622219738838497,0.6623948542665565,0.6625676188271354,0.6627402676431821,0.6629128007922401,0.6630852183518013,0.6632575203993054,0.6634297070121403,0.6636017782676422,0.6637737342430953,0.6639455750157319,0.664117300662733,0.6642889112612275,0.6644604068882927,0.6646317876209543,0.6648030535361866,0.6649742047109121,0.6651452412220016,0.6653161631462748,0.6654869705604998,0.665657663541393,0.6658282421656198,0.6659987065097939,0.666169056650478,0.6663392926641832,0.6665094146273696,0.6666794226164457,0.6668493167077694,0.6670190969776468,0.6671887635023335,0.6673583163580333,0.6675277556208997,0.6676970813670347,0.6678662936724893,0.6680353926132637,0.6682043782653073,0.6683732507045185,0.6685420100067447,0.6687106562477828,0.6688791895033785,0.6690476098492273,0.6692159173609734,0.6693841121142108,0.6695521941844825,0.6697201636472812,0.6698880205780487,0.6700557650521767,0.6702233971450059,0.6703909169318267,0.6705583244878793,0.6707256198883532,0.6708928032083876,0.6710598745230715,0.6712268339074433,0.6713936814364914,0.6715604171851538,0.6717270412283185,0.6718935536408231,0.6720599544974553,0.6722262438729523,0.6723924218420015,0.6725584884792405,0.6727244438592563,0.6728902880565866,0.6730560211457185,0.6732216432010897,0.6733871542970878,0.6735525545080507,0.6737178439082665,0.6738830225719732,0.6740480905733595,0.6742130479865641,0.6743778948856762,0.6745426313447354,0.6747072574377313,0.6748717732386045,0.6750361788212457,0.6752004742594961,0.6753646596271478,0.6755287349979427,0.6756927004455743,0.6758565560436859,0.6760203018658718,0.676183937985677,0.6763474644765972,0.6765108814120788,0.6766741888655191,0.6768373869102662,0.677000475619619,0.6771634550668273,0.6773263253250918,0.6774890864675644,0.6776517385673477,0.6778142816974954,0.6779767159310123,0.6781390413408542,0.6783012579999282,0.6784633659810925,0.6786253653571562,0.6787872562008802,0.6789490385849761,0.6791107125821071,0.6792722782648875,0.6794337357058831,0.6795950849776111,0.6797563261525401,0.6799174593030901,0.6800784845016326,0.6802394018204906,0.6804002113319386,0.6805609131082029,0.6807215072214612,0.6808819937438428,0.681042372747429,0.6812026443042524,0.6813628084862975,0.6815228653655008,0.6816828150137502,0.6818426575028858,0.6820023929046992,0.6821620212909344,0.6823215427332869,0.6824809573034044,0.6826402650728864,0.6827994661132847,0.6829585604961028,0.6831175482927967,0.6832764295747742,0.6834352044133954,0.6835938728799729,0.6837524350457708,0.6839108909820061,0.6840692407598478,0.6842274844504173,0.6843856221247883,0.6845436538539869,0.6847015797089916,0.6848593997607335,0.685017114080096,0.685174722737915,0.6853322258049791,0.6854896233520293,0.6856469154497594,0.6858041021688157,0.6859611835797972,0.6861181597532555,0.6862750307596952,0.6864317966695733,0.6865884575533,0.6867450134812381,0.6869014645237032,0.6870578107509638,0.6872140522332416,0.6873701890407109,0.6875262212434992,0.6876821489116869,0.6878379721153076,0.6879936909243479,0.6881493054087475,0.6883048156383992,0.688460221683149,0.6886155236127962,0.6887707214970932,0.6889258154057457,0.6890808054084129,0.689235691574707,0.6893904739741937,0.6895451526763923,0.6896997277507751,0.6898541992667684,0.6900085672937515,0.6901628319010574,0.6903169931579728,0.6904710511337377,0.690625005897546,0.6907788575185451,0.6909326060658358,0.6910862516084731,0.6912397942154656,0.6913932339557752,0.6915465708983184,0.6916998051119648,0.6918529366655384,0.6920059656278167,0.6921588920675313,0.6923117160533679,0.6924644376539659,0.6926170569379189,0.6927695739737743,0.6929219888300339,0.6930743015751534,0.6932265122775427,0.6933786210055659,0.6935306278275413,0.6936825328117413,0.6938343360263928,0.6939860375396767,0.6941376374197284,0.6942891357346377,0.6944405325524486,0.6945918279411598,0.6947430219687241,0.6948941147030488,0.6950451062119962,0.6951959965633826,0.695346785824979,0.695497474064511,0.6956480613496591,0.6957985477480579,0.6959489333272973,0.6960992181549215,0.6962494022984296,0.6963994858252753,0.6965494688028675,0.6966993512985696,0.6968491333797001,0.6969988151135321,0.6971483965672939,0.6972978778081685,0.6974472589032943,0.6975965399197643,0.6977457209246268,0.6978948019848852,0.6980437831674977,0.698192664539378,0.6983414461673948,0.6984901281183721,0.6986387104590889,0.6987871932562797,0.6989355765766342,0.6990838604867975,0.6992320450533699,0.6993801303429072,0.6995281164219207,0.6996760033568769,0.6998237912141978,0.6999714800602611,0.7001190699613998,0.7002665609839027,0.700413953194014,0.7005612466579336,0.7007084414418171,0.7008555376117757,0.7010025352338761,0.7011494343741412,0.7012962350985494],"y":[0.3,-0.03361323754098372,0.26160397649113626,0.0003209250979519518,0.23152764209742147,0.02689241778860199,0.2079671596567174,0.04769759425908382,0.18950979327702383,0.06398669273693891,0.175049111210476,0.07673887942754495,0.16371857412552013,0.0867210213440263,0.15483952252346134,0.09453371623028901,0.14788044133033773,0.10064734244695944,0.1424250565888151,0.10543029265483592,0.13814734930695172,0.10917108597586869,0.1347919867115501,0.11209568587745498,0.13215899632453815,0.1143810632573274,0.1300917629492342,0.11616581883070967,0.12846762810550696,0.11755850241090882,0.1271905276605191,0.11864412843436212,0.1261852257410506,0.11948927881372261,0.12539279882755439,0.12014609940927522,0.12476709897018053,0.12065543000025125,0.12427198383717686,0.1210492556273778,0.12387914733391601,0.12135262644443258,0.12356642057905459,0.1215851613147091,0.12331644125673624,0.1217622254032762,0.12311561147491215,0.12189585244805377,0.12295328157695308,0.1219954670674964,0.12282111091619864,0.12206845046020562,0.12271256722499682,0.1221205834516514,0.12262253452869029,0.12215639348114773,0.12254700607023736,0.12217942635639961,0.12248284381377693,0.12219245908723787,0.12242759009173794,0.12219766657353323,0.12237932008992375,0.12219675215245555,0.1223365263162328,0.12219104984096063,0.1222980281184428,0.12218160441044051,0.1222629008200129,0.12216923409988519,0.1222304202203985,0.12215457973180939,0.12220001912860153,0.12213814317904631,0.12217125332095526,0.12212031749131382,0.12214377487981583,0.12210141048985053,0.12211731131285959,0.12208166324635016,0.12209164919965716,0.12206126455536212,0.12206662138393623,0.1220403622688399,0.12204209694277066,0.12201907217317454,0.12201797333061275,0.12199748494154278,0.12199417022662713,0.12197567157887285,0.12197062471602298,0.12195368768625292,0.1219472875161518,0.12193157680074637,0.1219241200208488,0.12190937301108062,0.12190109198560903,0.12188710300621211,0.12187817971465505,0.12186478767972889,0.12185536464107805,0.12184244338639262,0.12183263221482714,0.12182008292624191,0.12180997103180094,0.12179771631532574,0.1217873721517662,0.12177535138932952,0.1217648285641627,0.12175299427632516,0.12174233476973037,0.12173064976702105,0.12171988645284651,0.12170832160473607,0.1216974802249055,0.12168601271250214,0.12167511342333809,0.12166372537092751,0.1216527839542065,0.1216414613574963,0.12163049016892741,0.12161922205566557,0.12160823076772341,0.12159700854030833,0.12158600472400782,0.12157482164463126,0.12156381122516408,0.12155266201258334,0.12154164962616494,0.12153053013990153,0.12151951941324803,0.12150842640625717,0.1214974201754669,0.12148635110043224,0.1214753515824104,0.12146430444003728,0.12145331336675275,0.12144228658695386,0.12143130531058706,0.12142029765942908,0.12140932723472196,0.12139833774154758,0.1213873789902989,0.12137640689064999,0.1213654604522269,0.12135450514314289,0.12134357151404067,0.12133263251904891,0.1213217120838738,0.1213107890255706,0.12129988208130464,0.12128897465988124,0.12127808143488643,0.1212671894113108,0.12125631008021262,0.12124543326305769,0.12123456795840182,0.12122370619352965,0.12121285501491107,0.12120200817739361,0.12119117119860663,0.12118033918639813,0.12116951646103594,0.12115869919001737,0.1211478907558576,0.12113708815595546,0.12112629403839495,0.12111550605054157,0.12110472626528622,0.12109395283903918,0.1210831873942108,0.12107242848588833,0.1210616773836746,0.12105093295489543,0.12104019619284202,0.1210294662093818,0.12101874378140444,0.1210080282122999,0.12099732010947707,0.12098661892632456,0.12097592513751815,0.12096523831392428,0.12095455882626573,0.12094388633741708,0.12093322113668813,0.12092256295901421,0.1209119120299451,0.12090126814085442,0.12089063146735744,0.12088000184503066,0.12086937941038302,0.12085876403361122,0.12084815582059821,0.12083755466865602,0.12082696065968321,0.12081637371222946,0.12080579388941048,0.12079522112641046,0.12078465547163593,0.1207740968733003,0.12076354536829159,0.12075300091502882,0.12074246354138023,0.12073193321375912,0.12072140995297093,0.12071089373169133,0.12070038456519566,0.12068988243106553,0.12067938734024657,0.12066889927416401,0.12065841824037385,0.12064794422331301,0.1206374772278841,0.1206270172408841,0.1206165642651389,0.12060611828929521,0.12059567931455388,0.12058524733101152,0.12057482233859769,0.12056440432854597,0.12055399329979154,0.12054358924445979,0.12053319216070849,0.12052280204136284,0.1205124188839731,0.12050204268191386,0.12049167343226103,0.12048131112882075,0.12047095576829873,0.12046060734484056,0.1204502658548633,0.12043993129277966,0.12042960365478213,0.12041928293549387,0.1204089691309328,0.12039866223588831,0.12038836224624294,0.1203780691569176,0.12036778296369008,0.12035750366158571,0.12034723124630155,0.12033696571294604,0.1203267070571543,0.12031645527410129,0.1203062103593749,0.12029597230820352,0.1202857411161394,0.12027551677845408,0.12026529929067321,0.1202550886481035,0.12024488484625111,0.12023468788045151,0.12022449774619703,0.12021431443884698,0.12020413795388415,0.12019396828668784,0.12018380543273467,0.12017364938742096,0.12016350014621986,0.12015335770454225,0.12014322205785989,0.12013309320159643,0.12012297113122383,0.12011285584217704,0.12010274732992955,0.12009264558992644,0.12008255061764367,0.1200724624085356,0.12006238095808149,0.1200523062617442,0.12004223831500686,0.1200321771133404,0.12002212265223226,0.12001207492716091,0.12000203393361858,0.11999199966709091,0.11998197212307514,0.11997195129706388,0.11996193718455961,0.11995192978106164,0.1199419290820779,0.1199319350831143,0.11992194777968421,0.11991196716730007,0.11990199324148079,0.1198920259977453,0.1198820654316181,0.1198721115386244,0.11986216431429449,0.11985222375415978,0.11984228985375638,0.11983236260862178,0.11982244201429794,0.11981252806632853,0.11980262076026132,0.11979272009164604,0.11978282605603635,0.119772938648988,0.11976305786606055,0.11975318370281578,0.11974331615481912,0.11973345521763834,0.11972360088684478,0.1197137531580122,0.11970391202671782,0.11969407748854137,0.11968424953906592,0.1196744281738772,0.11966461338856417,0.11965480517871846,0.11964500353993496,0.11963520846781117,0.11962541995794793,0.1196156380059486,0.11960586260741989,0.11959609375797116,0.11958633145321484,0.11957657568876633,0.11956682646024382,0.11955708376326864,0.11954734759346483,0.1195376179464596,0.11952789481788285,0.11951817820336764,0.11950846809854972,0.11949876449906802,0.11948906740056407,0.11947937679868272,0.11946969268907133,0.11946001506738055,0.11945034392926358,0.11944067927037685,0.11943102108637951,0.11942136937293371,0.11941172412570442,0.11940208534035958,0.11939245301257007,0.11938282713800949,0.11937320771235464,0.11936359473128487,0.11935398819048273,0.11934438808563341,0.11933479441242527,0.11932520716654918,0.11931562634369937,0.11930605193957242,0.11929648394986835,0.11928692237028952,0.11927736719654167,0.11926781842433302,0.11925827604937489,0.1192487400673814,0.11923921047406953,0.1192296872651592,0.11922017043637305,0.1192106599834368,0.1192011559020788,0.11919165818803051,0.11918216683702598,0.11917268184480234,0.11916320320709949,0.11915373091966013,0.11914426497822991,0.11913480537855728,0.11912535211639355,0.11911590518749286,0.11910646458761222,0.11909703031251145,0.11908760235795329,0.11907818071970314,0.11906876539352951,0.11905935637520342,0.11904995366049909,0.11904055724519313,0.1190311671250655,0.11902178329589848,0.11901240575347757,0.11900303449359081,0.11899366951202929,0.1189843108045867,0.11897495836705978,0.11896561219524786,0.11895627228495327,0.11894693863198103,0.11893761123213904,0.11892829008123793,0.11891897517509127,0.11890966650951529,0.11890036408032911,0.11889106788335461,0.11888177791441654,0.11887249416934231,0.1188632166439623,0.1188539453341095,0.11884468023561988,0.11883542134433202,0.11882616865608743,0.11881692216673033,0.11880768187210772,0.11879844776806946,0.11878921985046806,0.11877999811515896,0.11877078255800023,0.11876157317485284,0.11875236996158048,0.11874317291404954,0.11873398202812933,0.1187247972996918,0.11871561872461173,0.11870644629876664,0.11869728001803681,0.11868811987830531,0.11867896587545793,0.11866981800538323,0.11866067626397252,0.1186515406471199,0.11864241115072213,0.11863328777067889,0.11862417050289233,0.11861505934326769,0.1186059542877126,0.11859685533213779,0.11858776247245638,0.11857867570458452,0.11856959502444085,0.118560520427947,0.11855145191102708,0.11854238946960811,0.11853333309961975,0.11852428279699445,0.11851523855766728,0.11850620037757617,0.11849716825266167,0.11848814217886706,0.11847912215213845,0.11847010816842445,0.1184611002236766,0.11845209831384901,0.11844310243489864,0.11843411258278494,0.11842512875347033,0.11841615094291967,0.11840717914710083,0.11839821336198401,0.1183892535835425,0.11838029980775192,0.11837135203059092,0.11836241024804055,0.1183534744560848,0.11834454465071015,0.11833562082790594,0.11832670298366404,0.11831779111397914,0.1183088852148485,0.11829998528227216,0.1182910913122528,0.11828220330079572,0.118273321243909,0.11826444513760331,0.11825557497789209,0.11824671076079128,0.1182378524823197,0.11822900013849867,0.11822015372535229,0.1182113132389072,0.11820247867519286,0.11819365003024122,0.11818482730008707,0.11817601048076766,0.1181671995683231,0.11815839455879593,0.11814959544823157,0.11814080223267788,0.11813201490818558,0.1181232334708078,0.11811445791660051,0.11810568824162224,0.11809692444193415,0.11808816651360007,0.11807941445268648,0.11807066825526244,0.11806192791739965,0.11805319343517254,0.118044464804658,0.11803574202193574,0.11802702508308795,0.11801831398419949,0.11800960872135786,0.11800090929065317,0.11799221568817818,0.11798352791002814,0.11797484595230116,0.11796616981109767,0.11795749948252096,0.11794883496267677,0.11794017624767357,0.1179315233336223,0.11792287621663664,0.1179142348928328,0.11790559935832957,0.11789696960924846,0.11788834564171338,0.11787972745185112,0.11787111503579069,0.1178625083896641,0.11785390750960556,0.11784531239175226,0.1178367230322436,0.11782813942722191,0.11781956157283177,0.11781098946522069,0.11780242310053843,0.11779386247493759,0.11778530758457317,0.1177767584256029,0.11776821499418692,0.11775967728648808,0.11775114529867171,0.11774261902690575,0.11773409846736077,0.11772558361620969,0.11771707446962831,0.11770857102379466,0.11770007327488968,0.11769158121909647,0.11768309485260112,0.11767461417159182,0.1176661391722598,0.11765766985079834,0.11764920620340376,0.11764074822627447,0.11763229591561182,0.11762384926761942,0.1176154082785036,0.11760697294447311,0.11759854326173934,0.11759011922651613,0.11758170083501997,0.11757328808346976,0.11756488096808698,0.1175564794850957,0.11754808363072239,0.11753969340119626,0.11753130879274874,0.11752292980161415,0.11751455642402894,0.11750618865623248,0.11749782649446629,0.11748946993497471,0.11748111897400437,0.11747277360780459,0.11746443383262707,0.1174560996447261,0.11744777104035847,0.11743944801578339,0.11743113056726277,0.11742281869106075,0.1174145123834443,0.11740621164068253,0.11739791645904742,0.11738962683481308,0.11738134276425649,0.11737306424365676,0.11736479126929582,0.11735652383745777,0.11734826194442956,0.11734000558650023,0.11733175475996171,0.11732350946110805,0.11731526968623611,0.11730703543164486,0.11729880669363621,0.11729058346851404,0.1172823657525852,0.11727415354215856,0.11726594683354588,0.11725774562306103,0.1172495499070206,0.11724135968174353,0.11723317494355123,0.11722499568876765,0.11721682191371914,0.11720865361473447,0.11720049078814498,0.1171923334302844,0.1171841815374889,0.11717603510609716,0.11716789413245025,0.11715975861289178,0.11715162854376768,0.1171435039214265,0.117135384742219,0.11712727100249871,0.1171191626986212,0.11711105982694492,0.11710296238383032,0.11709487036564074,0.11708678376874146,0.11707870258950072,0.11707062682428873,0.11706255646947844,0.11705449152144506,0.11704643197656636,0.11703837783122244,0.1170303290817958,0.1170222857246715,0.11701424775623687,0.11700621517288179,0.11699818797099842,0.11699016614698152,0.11698214969722805,0.11697413861813756,0.11696613290611192,0.11695813255755548,0.11695013756887489,0.11694214793647933,0.11693416365678026,0.1169261847261917,0.11691821114112992,0.11691024289801369,0.11690227999326412,0.11689432242330475,0.11688637018456154,0.11687842327346275,0.11687048168643918,0.11686254541992383,0.11685461447035234,0.11684668883416242,0.11683876850779452,0.11683085348769115,0.11682294377029746,0.11681503935206079,0.116807140229431,0.11679924639886025,0.11679135785680307,0.11678347459971643,0.1167755966240596,0.11676772392629432,0.11675985650288456,0.11675199435029682,0.11674413746499976,0.11673628584346471,0.11672843948216502,0.11672059837757666,0.11671276252617784,0.11670493192444914,0.11669710656887355,0.11668928645593639,0.11668147158212526,0.11667366194393028,0.11666585753784371,0.11665805836036036,0.11665026440797725,0.1166424756771938,0.11663469216451183,0.11662691386643534,0.1166191407794709,0.11661137290012716,0.11660361022491542,0.11659585275034892,0.11658810047294373,0.1165803533892177,0.11657261149569155,0.11656487478888787,0.11655714326533198,0.11654941692155116,0.11654169575407532,0.11653397975943651,0.11652626893416913,0.11651856327481001,0.11651086277789818,0.11650316743997502,0.11649547725758429,0.11648779222727193,0.11648011234558636,0.11647243760907817,0.11646476801430035,0.11645710355780818,0.1164494442361592,0.11644179004591333,0.11643414098363276,0.11642649704588198,0.11641885822922773,0.11641122453023921,0.1164035959454877,0.11639597247154697,0.11638835410499296,0.11638074084240399,0.11637313268036058,0.11636552961544565,0.1163579316442443,0.11635033876334401,0.11634275096933445,0.11633516825880771,0.11632759062835803,0.11632001807458199,0.11631245059407848,0.11630488818344854,0.11629733083929575,0.11628977855822559,0.11628223133684622,0.11627468917176771,0.11626715205960267,0.11625961999696582,0.11625209298047423,0.11624457100674722,0.11623705407240628,0.11622954217407537,0.11622203530838049,0.11621453347195007,0.11620703666141466,0.11619954487340718,0.11619205810456275,0.11618457635151876,0.11617709961091484,0.11616962787939285,0.11616216115359701,0.11615469943017362,0.11614724270577138,0.11613979097704111,0.116132344240636,0.11612490249321135,0.1161174657314248,0.1161100339519362,0.11610260715140762,0.11609518532650337,0.11608776847389005,0.11608035659023638,0.11607294967221345,0.11606554771649442,0.11605815071975488,0.11605075867867244,0.11604337158992709,0.11603598945020095,0.1160286122561784,0.11602124000454608,0.11601387269199274,0.11600651031520952,0.11599915287088958,0.11599180035572842,0.11598445276642372,0.11597711009967543,0.11596977235218554,0.11596243952065853,0.11595511160180073,0.11594778859232106,0.11594047048893032,0.11593315728834172,0.11592584898727058,0.11591854558243442,0.11591124707055307,0.1159039534483483,0.11589666471254446,0.11588938085986769,0.11588210188704666,0.11587482779081194,0.1158675585678966,0.11586029421503556,0.11585303472896627,0.11584578010642806,0.11583853034416267,0.11583128543891386,0.11582404538742774,0.11581681018645239,0.1158095798327383,0.11580235432303794,0.11579513365410608,0.11578791782269962,0.11578070682557759,0.11577350065950133,0.1157662993212341,0.11575910280754166,0.11575191111519163,0.115744724240954,0.1157375421816008,0.11573036493390634,0.11572319249464692,0.11571602486060124,0.11570886202854985,0.11570170399527582,0.11569455075756403,0.11568740231220175,0.11568025865597828,0.11567311978568513,0.11566598569811593,0.11565885639006651,0.11565173185833472,0.11564461209972075,0.11563749711102671,0.11563038688905707,0.11562328143061826,0.11561618073251897,0.11560908479156998,0.11560199360458419,0.11559490716837668,0.11558782547976465,0.11558074853556738,0.11557367633260637,0.11556660886770517,0.11555954613768953,0.11555248813938723,0.11554543486962832,0.1155383863252448,0.11553134250307091,0.115524303399943,0.11551726901269951,0.115510239338181,0.11550321437323015,0.11549619411469178,0.1154891785594128,0.11548216770424222,0.11547516154603121,0.11546816008163295,0.11546116330790289,0.1154541712216984,0.11544718381987912,0.11544020109930667,0.11543322305684486,0.11542624968935955,0.1154192809937187,0.11541231696679242,0.11540535760545283,0.11539840290657427,0.11539145286703299,0.11538450748370757,0.11537756675347843,0.11537063067322831,0.11536369923984184,0.11535677245020591,0.11534985030120935,0.1153429327897432,0.11533601991270044,0.11532911166697633,0.11532220804946795,0.11531530905707474,0.11530841468669799,0.11530152493524118,0.11529463979960985,0.11528775927671159,0.1152808833634561,0.1152740120567551,0.11526714535352242,0.11526028325067389,0.11525342574512758,0.11524657283380334,0.1152397245136234,0.1152328807815118,0.11522604163439476,0.11521920706920057,0.1152123770828595,0.11520555167230398,0.11519873083446837,0.11519191456628923,0.115185102864705,0.11517829572665637,0.11517149314908587,0.1151646951289383,0.11515790166316027,0.11515111274870066,0.11514432838251021,0.11513754856154185,0.11513077328275038,0.11512400254309293,0.11511723633952824,0.11511047466901758,0.11510371752852377,0.11509696491501212,0.11509021682544958,0.11508347325680543,0.11507673420605075,0.11506999967015887,0.1150632696461049,0.11505654413086626,0.11504982312142212,0.11504310661475387,0.11503639460784483,0.11502968709768034,0.11502298408124782,0.11501628555553665,0.11500959151753827,0.11500290196424606,0.11499621689265556,0.11498953629976412,0.11498286018257131,0.1149761885380785,0.11496952136328933,0.11496285865520915,0.1149562004108456,0.11494954662720804,0.11494289730130813,0.11493625243015927,0.11492961201077707,0.11492297604017895,0.11491634451538453,0.11490971743341521,0.1149030947912946,0.11489647658604811,0.11488986281470329,0.11488325347428958,0.11487664856183852,0.11487004807438352,0.11486345200896002,0.11485686036260553,0.11485027313235935,0.11484369031526304,0.11483711190835982,0.1148305379086952,0.11482396831331643,0.11481740311927292,0.11481084232361587,0.11480428592339868,0.11479773391567648,0.11479118629750658,0.11478464306594813,0.11477810421806232,0.11477156975091224,0.11476503966156312,0.11475851394708184,0.11475199260453761,0.1147454756310013,0.11473896302354597,0.11473245477924646,0.11472595089517969,0.11471945136842449,0.11471295619606167,0.11470646537517395,0.11469997890284608,0.11469349677616467,0.11468701899221839,0.11468054554809771,0.11467407644089528,0.1146676116677054,0.11466115122562463,0.11465469511175115,0.11464824332318543,0.11464179585702958,0.11463535271038786,0.11462891388036632,0.11462247936407308,0.11461604915861807,0.11460962326111329,0.11460320166867255,0.11459678437841168,0.11459037138744839,0.11458396269290239,0.11457755829189518,0.11457115818155042,0.11456476235899339,0.11455837082135166,0.11455198356575429,0.11454560058933275,0.11453922188921996,0.1145328474625512,0.11452647730646325,0.11452011141809516,0.11451374979458763,0.11450739243308354,0.11450103933072739,0.11449469048466587,0.1144883458920473,0.11448200555002225,0.11447566945574285,0.1144693376063634,0.11446300999903994,0.11445668663093055,0.11445036749919509,0.1144440526009954,0.1144377419334952,0.11443143549386012,0.11442513327925767,0.11441883528685724,0.11441254151383022,0.1144062519573497,0.11439996661459093,0.11439368548273077,0.11438740855894818,0.1143811358404239,0.11437486732434064,0.11436860300788292,0.11436234288823718,0.11435608696259174,0.11434983522813681,0.11434358768206451,0.11433734432156875,0.11433110514384548,0.11432487014609227,0.11431863932550893,0.11431241267929673,0.11430619020465921,0.11429997189880149,0.11429375775893072,0.11428754778225585,0.11428134196598777,0.11427514030733911,0.11426894280352458,0.11426274945176046,0.11425656024926525,0.11425037519325891,0.11424419428096373,0.11423801750960334],"z":[15.852941422131149,12.647005410183102,10.136159687885552,8.169697369982421,6.6295872787897565,5.423389318539712,4.478705911967834,3.7388364558289826,3.159373961241957,2.705539597844701,2.3500951521943,2.07170809918979,1.8536711526419083,1.682899438136516,1.549145095156312,1.444382166255895,1.3623248522832225,1.2980502177161737,1.2477026996279312,1.2082626839219037,1.1773652580085772,1.153158260854372,1.1341911100921265,1.1193277332249227,1.1076783767631855,1.0985462002516229,1.0913854495810495,1.0857686990075304,1.081361195635362,1.0779007664340485,1.0751820817412907,1.0730443306948638,1.0713615688316096,1.0700351584842043,1.0689878482230404,1.0681591359715503,1.067501637473657,1.0669782421366356,1.0665598855335727,1.0662238048635717,1.0659521726566106,1.065731026713431,1.065549432051901,1.065398824557048,1.0652724969384615,1.0651651961405029,1.0650728080405374,1.0649921105097262,1.0649205800142678,1.064856240148626,1.0647975430092365,1.0647432762883158,1.0646924905112516,1.0646444420501084,1.0645985484927365,1.0645543536885802,1.0645115003731334,1.0644697087278465,1.0644287595886062,1.064388481294881,1.0643487393901971,1.0643094285557078,1.0642704662926976,1.0642317879748366,1.0641933429731645,1.0641550916212588,1.0641170028383964,1.0640790522680528,1.0640412208200078,1.0640034935285434,1.0639658586582013,1.0639283070034338,1.0638908313400968,1.063853425995865,1.063816086513797,1.0637788093888356,1.0637415918614428,1.0637044317559827,1.063667327354139,1.0636302772957769,1.0635932805013082,1.0635563361108795,1.063519443436749,1.0634826019259942,1.0634458111312957,1.0634090706880586,1.0633723802964983,1.0633357397076104,1.06329914871217,1.0632626071321376,1.0632261148139097,1.0631896716230407,1.0631532774401091,1.0631169321574767,1.0630806356767593,1.063044387906829,1.0630081887622664,1.0629720381621421,1.0629359360290596,1.0628998822884075,1.062863876867778,1.0628279196965018,1.0627920107052977,1.0627561498259823,1.0627203369912588,1.0626845721345348,1.0626488551897955,1.0626131860914925,1.062577564774463,1.062541991173864,1.0625064652251226,1.0624709868638953,1.0624355560260352,1.0624001726475714,1.0623648366646843,1.0623295480136972,1.0622943066310568,1.0622591124533318,1.062223965417197,1.062188865459438,1.0621538125169359,1.062118806526672,1.0620838474257188,1.062048935151244,1.0620140696405034,1.0619792508308419,1.0619444786596912,1.0619097530645722,1.0618750739830876,1.0618404413529303,1.0618058551118734,1.0617713151977777,1.0617368215485872,1.0617023741023284,1.0616679727971143,1.0616336175711374,1.0615993083626754,1.061565045110089,1.061530827751822,1.0614966562263994,1.0614625304724292,1.0614284504286036,1.061394416033693,1.061360427226554,1.0613264839461225,1.061292586131417,1.0612587337215384,1.061224926655668,1.0611911648730699,1.061157448313088,1.0611237769151496,1.0610901506187598,1.0610565693635088,1.061023033089064,1.0609895417351771,1.060956095241677,1.060922693548476,1.060889336595565,1.0608560243230165,1.0608227566709842,1.0607895335796977,1.0607563549894723,1.0607232208406998,1.0606901310738515,1.0606570856294812,1.0606240844482184,1.0605911274707764,1.0605582146379449,1.0605253458905937,1.0604925211696727,1.0604597404162104,1.060427003571313,1.0603943105761668,1.0603616613720377,1.0603290559002685,1.0602964941022823,1.0602639759195782,1.0602315012937364,1.0601990701664135,1.060166682479346,1.0601343381743453,1.0601020371933054,1.0600697794781926,1.0600375649710563,1.0600053936140204,1.0599732653492862,1.059941180119134,1.0599091378659196,1.059877138532079,1.0598451820601213,1.059813268392636,1.059781397472287,1.0597495692418168,1.0597177836440452,1.0596860406218644,1.059654340118248,1.0596226820762442,1.0595910664389772,1.0595594931496466,1.0595279621515288,1.0594964733879777,1.0594650268024206,1.059433622338363,1.0594022599393822,1.0593709395491364,1.0593396611113548,1.059308424569845,1.059277229868487,1.059246076951239,1.0592149657621317,1.0591838962452713,1.059152868344842,1.0591218820050976,1.0590909371703703,1.059060033785065,1.0590291717936635,1.0589983511407177,1.0589675717708584,1.0589368336287883,1.0589061366592827,1.058875480807194,1.058844866017447,1.0588142922350388,1.058783759405044,1.058753267472606,1.0587228163829467,1.058692406081356,1.0586620365132011,1.058631707623922,1.0586014193590296,1.0585711716641089,1.0585409644848198,1.058510797766891,1.058480671456128,1.058450585498405,1.0584205398396729,1.0583905344259499,1.0583605692033318,1.0583306441179827,1.0583007591161406,1.0582709141441167,1.0582411091482904,1.058211344075116,1.0581816188711184,1.0581519334828957,1.058122287857114,1.0580926819405154,1.058063115679908,1.0580335890221766,1.0580041019142736,1.0579746543032245,1.057945246136123,1.0579158773601376,1.0578865479225035,1.0578572577705294,1.0578280068515942,1.0577987951131453,1.0577696225027033,1.0577404889678568,1.0577113944562662,1.0576823389156604,1.0576533222938405,1.0576243445386755,1.0575954055981045,1.057566505420139,1.0575376439528557,1.0575088211444037,1.0574800369430013,1.0574512912969356,1.0574225841545635,1.057393915464311,1.0573652851746724,1.0573366932342114,1.0573081395915602,1.057279624195423,1.0572511469945671,1.057222707937833,1.057194306974127,1.0571659440524255,1.0571376191217732,1.057109332131282,1.0570810830301338,1.0570528717675745,1.0570246982929226,1.0569965625555628,1.0569684645049469,1.0569404040905952,1.056912381262095,1.0568843959691017,1.056856448161339,1.0568285377885938,1.0568006648007264,1.0567728291476608,1.056745030779387,1.0567172696459635,1.0566895456975183,1.05666185888424,1.0566342091563907,1.0566065964642928,1.0565790207583396,1.056551481988991,1.0565239801067707,1.0564965150622683,1.056469086806144,1.0564416952891194,1.056414340461983,1.0563870222755916,1.056359740680867,1.056332495628795,1.0563052870704268,1.0562781149568825,1.0562509792393457,1.0562238798690644,1.0561968167973517,1.0561697899755893,1.0561427993552222,1.056115844887757,1.056088926524771,1.0560620442179023,1.0560351979188545,1.056008387579399,1.0559816131513666,1.0559548745866567,1.0559281718372315,1.055901504855118,1.0558748735924068,1.0558482780012548,1.0558217180338774,1.0557951936425622,1.055768704779654,1.0557422513975643,1.055715833448769,1.0556894508858043,1.0556631036612751,1.0556367917278444,1.055610515038242,1.05558427354526,1.0555580672017557,1.0555318959606457,1.0555057597749118,1.055479658597599,1.055453592381814,1.0554275610807295,1.055401564647576,1.055375603035651,1.055349676198312,1.0553237840889804,1.0552979266611373,1.0552721038683313,1.0552463156641683,1.055220562002318,1.0551948428365128,1.0551691581205462,1.0551435078082756,1.0551178918536162,1.0550923102105483,1.055066762833113,1.055041249675412,1.0550157706916106,1.0549903258359328,1.0549649150626672,1.0549395383261595,1.0549141955808197,1.054888886781118,1.0548636118815853,1.0548383708368145,1.0548131636014566,1.054787990130227,1.0547628503778996,1.0547377442993089,1.054712671849349,1.0546876329829775,1.0546626276552105,1.0546376558211221,1.0546127174358513,1.0545878124545944,1.054562940832606,1.054538102525205,1.0545132974877662,1.0544885256757268,1.0544637870445812,1.0544390815498867,1.0544144091472563,1.054389769792366,1.0543651634409488,1.0543405900487988,1.0543160495717676,1.0542915419657675,1.054267067186766,1.054242625190797,1.0542182159339453,1.0541938393723602,1.0541694954622476,1.0541451841598704,1.0541209054215528,1.0540966592036782,1.0540724454626835,1.0540482641550684,1.0540241152373915,1.0539999986662651,1.053975914398363,1.0539518623904174,1.0539278425992162,1.0539038549816064,1.0538798994944931,1.0538559760948376,1.0538320847396616,1.0538082253860428,1.0537843979911143,1.0537606025120707,1.0537368389061612,1.0537131071306922,1.053689407143029,1.0536657389005932,1.0536421023608629,1.053618497481373,1.0535949242197167,1.0535713825335424,1.0535478723805565,1.0535243937185197,1.0535009465052552,1.0534775306986348,1.0534541462565923,1.0534307931371145,1.0534074712982484,1.0533841806980917,1.0533609212948052,1.0533376930465979,1.0533144959117415,1.0532913298485598,1.0532681948154334,1.0532450907707993,1.0532220176731495,1.0531989754810305,1.0531759641530474,1.0531529836478575,1.0531300339241754,1.053107114940771,1.053084226656468,1.053061369030146,1.0530385420207402,1.05301574558724,1.0529929796886912,1.0529702442841928,1.0529475393328995,1.0529248647940188,1.0529022206268162,1.05287960679061,1.0528570232447714,1.05283446994873,1.052811946861965,1.052789453944014,1.0527669911544648,1.0527445584529638,1.052722155799209,1.0526997831529514,1.052677440473998,1.052655127722209,1.0526328448574966,1.0526105918398292,1.0525883686292277,1.0525661751857676,1.0525440114695765,1.052521877440834,1.0524997730597774,1.052477698286694,1.052455653081924,1.0524336374058627,1.0524116512189576,1.0523896944817093,1.0523677671546703,1.0523458691984473,1.0523240005736991,1.0523021612411372,1.0522803511615264,1.0522585702956824,1.052236818604475,1.0522150960488268,1.0521934025897108,1.052171738188153,1.0521501028052325,1.05212849640208,1.0521069189398786,1.0520853703798614,1.0520638506833166,1.0520423598115831,1.0520208977260503,1.0519994643881594,1.0519780597594048,1.0519566838013317,1.0519353364755366,1.051914017743669,1.0518927275674255,1.05187146590856,1.051850232728873,1.0518290279902187,1.0518078516545,1.0517867036836737,1.0517655840397446,1.0517444926847717,1.0517234295808608,1.0517023946901722,1.0516813879749163,1.051660409397351,1.0516394589197888,1.0516185365045883,1.0515976421141637,1.0515767757109766,1.0515559372575374,1.0515351267164101,1.0515143440502062,1.051493589221589,1.051472862193271,1.0514521629280147,1.0514314913886327,1.0514108475379869,1.051390231338989,1.0513696427546015,1.0513490817478346,1.0513285482817512,1.0513080423194585,1.0512875638241193,1.0512671127589406,1.051246689087182,1.0512262927721505,1.0512059237772025,1.0511855820657443,1.0511652676012306,1.0511449803471646,1.0511247202671006,1.0511044873246378,1.051084281483428,1.0510641027071697,1.0510439509596103,1.0510238262045457,1.0510037284058213,1.0509836575273288,1.050963613533012,1.0509435963868567,1.0509236060529046,1.050903642495239,1.050883705677997,1.0508637955653573,1.0508439121215531,1.0508240553108605,1.0508042250976077,1.0507844214461664,1.050764644320958,1.0507448936864534,1.0507251695071669,1.0507054717476654,1.0506858003725599,1.050666155346508,1.0506465366342173,1.0506269442004403,1.0506073780099798,1.0505878380276825,1.0505683242184454,1.0505488365472087,1.0505293749789621,1.0505099394787418,1.0504905300116307,1.0504711465427583,1.0504517890373013,1.0504324574604822,1.050413151777571,1.0503938719538832,1.0503746179547822,1.0503553897456763,1.0503361872920223,1.0503170105593194,1.0502978595131163,1.0502787341190072,1.050259634342632,1.0502405601496765,1.0502215115058733,1.0502024883770003,1.0501834907288798,1.050164518527383,1.0501455717384243,1.0501266503279647,1.0501077542620103,1.0500888835066138,1.0500700380278725,1.0500512177919297,1.0500324227649722,1.0500136529132347,1.0499949082029951,1.0499761886005785,1.0499574940723535,1.0499388245847332,1.0499201801041784,1.049901560597192,1.0498829660303228,1.0498643963701642,1.0498458515833562,1.04982733163658,1.0498088364965645,1.0497903661300823,1.0497719205039486,1.0497534995850253,1.0497351033402198,1.0497167317364793,1.0496983847407995,1.0496800623202185,1.049661764441819,1.0496434910727288,1.0496252421801164,1.049607017731198,1.0495888176932322,1.0495706420335207,1.0495524907194103,1.049534363718291,1.0495162609975957,1.049498182524803,1.0494801282674333,1.0494620981930514,1.0494440922692638,1.049426110463721,1.0494081527441208,1.0493902190781987,1.049372309433735,1.0493544237785568,1.0493365620805282,1.0493187243075623,1.0493009104276103,1.0492831204086688,1.0492653542187773,1.049247611826018,1.0492298931985153,1.049212198304436,1.0491945271119905,1.049176879589431,1.0491592557050542,1.0491416554271953,1.0491240787242357,1.0491065255645977,1.0490889959167458,1.0490714897491864,1.0490540070304686,1.0490365477291839,1.049019111813966,1.0490016992534892,1.0489843100164715,1.0489669440716707,1.0489496013878896,1.0489322819339704,1.0489149856787963,1.0488977125912948,1.0488804626404329,1.048863235795222,1.0488460320247097,1.0488288512979906,1.048811693584198,1.048794558852506,1.048777447072133,1.048760358212334,1.0487432922424098,1.0487262491316989,1.0487092288495832,1.0486922313654847,1.0486752566488662,1.0486583046692304,1.0486413753961237,1.048624468799131,1.0486075848478773,1.0485907235120318,1.0485738847613002,1.0485570685654306,1.0485402748942132,1.0485235037174752,1.0485067550050873,1.0484900287269583,1.0484733248530398,1.0484566433533213,1.0484399841978342,1.0484233473566489,1.048406732799875,1.0483901404976643,1.0483735704202082,1.0483570225377377,1.0483404968205217,1.0483239932388728,1.0483075117631393,1.0482910523637134,1.048274615011022,1.048258199675537,1.0482418063277659,1.0482254349382558,1.0482090854775974,1.0481927579164156,1.0481764522253774,1.0481601683751889,1.048143906336595,1.048127666080379,1.048111447577366,1.0480952507984167,1.0480790757144336,1.048062922296357,1.0480467905151654,1.0480306803418784,1.0480145917475523,1.0479985247032824,1.0479824791802044,1.0479664551494903,1.047950452582352,1.0479344714500414,1.0479185117238459,1.0479025733750928,1.0478866563751488,1.0478707606954172,1.0478548863073391,1.047839033182399,1.0478232012921127,1.0478073906080374,1.047791601101769,1.0477758327449402,1.0477600855092226,1.047744359366324,1.0477286542879936,1.047712970246014,1.0476973072122087,1.0476816651584382,1.047666044056601,1.0476504438786316,1.0476348645965037,1.0476193061822292,1.0476037686078556,1.0475882518454676,1.0475727558671895,1.047557280645182,1.0475418261516427,1.0475263923588058,1.0475109792389439,1.0474955867643672,1.0474802149074214,1.0474648636404886,1.0474495329359919,1.0474342227663886,1.0474189331041708,1.0474036639218705,1.047388415192056,1.047373186887331,1.0473579789803398,1.0473427914437559,1.0473276242502978,1.0473124773727154,1.047297350783796,1.0472822444563636,1.0472671583632789,1.047252092477439,1.047237046771777,1.047222021219262,1.0472070157929003,1.0471920304657325,1.0471770652108374,1.0471621200013292,1.047147194810358,1.0471322896111082,1.0471174043768043,1.0471025390807,1.047087693696093,1.0470728681963108,1.0470580625547181,1.0470432767447158,1.0470285107397406,1.0470137645132644,1.0469990380387948,1.0469843312898741,1.046969644240081,1.0469549768630295,1.0469403291323682,1.0469257010217823,1.046911092504991,1.0468965035557494,1.0468819341478468,1.0468673842551095,1.0468528538513961,1.046838342910603,1.0468238514066606,1.0468093793135327,1.0467949266052203,1.0467804932557576,1.0467660792392148,1.046751684529696,1.04673730910134,1.0467229529283204,1.0467086159848447,1.0466942982451573,1.046679999683534,1.0466657202742875,1.046651459991764,1.0466372188103423,1.0466229967044394,1.0466087936485033,1.0465946096170167,1.0465804445844982,1.046566298525498,1.0465521714146044,1.0465380632264343,1.0465239739356433,1.046509903516918,1.046495851944981,1.0464818191945864,1.0464678052405239,1.0464538100576184,1.0464398336207237,1.046425875904732,1.0464119368845668,1.0463980165351872,1.0463841148315818,1.0463702317487773,1.0463563672618303,1.0463425213458346,1.0463286939759138,1.046314885127224,1.0463010947749602,1.0462873228943466,1.046273569460639,1.0462598344491307,1.046246117835144,1.0462324195940385,1.046218739701202,1.0462050781320595,1.0461914348620658,1.0461778098667103,1.0461642031215153,1.0461506146020347,1.0461370442838567,1.0461234921426,1.0461099581539182,1.0460964422934973,1.046082944537054,1.0460694648603406,1.046056003239137,1.0460425596492595,1.0460291340665573,1.0460157264669103,1.046002336826229,1.0459889651204606,1.0459756113255796,1.045962275417596,1.0459489573725511,1.0459356571665186,1.0459223747756032,1.0459091101759423,1.0458958633437054,1.0458826342550935,1.04586942288634,1.0458562292137097,1.045843053213499,1.0458298948620364,1.0458167541356835,1.0458036310108294,1.0457905254638995,1.0457774374713489,1.045764367009663,1.0457513140553607,1.045738278584992,1.0457252605751377,1.0457122600024087,1.0456992768434499,1.0456863110749355,1.0456733626735728,1.0456604316160976,1.0456475178792806,1.0456346214399195,1.0456217422748455,1.045608880360921,1.045596035675037,1.0455832081941197,1.045570397895122,1.0455576047550303,1.045544828750859,1.045532069859658,1.0455193280585033,1.0455066033245035,1.045493895634799,1.0454812049665583,1.0454685312969825,1.045455874603302,1.0454432348627787,1.0454306120527053,1.045418006150402,1.045405417133223,1.0453928449785508,1.0453802896637994,1.045367751166412,1.045355229463862,1.045342724533653,1.0453302363533208,1.0453177649004275,1.0453053101525693,1.0452928720873687,1.0452804506824807,1.0452680459155896,1.0452556577644103,1.0452432862066836,1.0452309312201868,1.0452185927827216,1.0452062708721213,1.0451939654662483,1.0451816765429969,1.045169404080288,1.045157148056073,1.045144908448334,1.0451326852350806,1.0451204783943546,1.0451082879042253,1.0450961137427908,1.0450839558881797,1.04507181431855,1.0450596890120878,1.0450475799470111,1.045035487101563,1.0450234104540193,1.045011349982683,1.0449993056658857,1.0449872774819913,1.0449752654093885,1.0449632694264976,1.0449512895117679,1.0449393256436743,1.0449273778007258,1.0449154459614545,1.0449035301044267,1.0448916302082327,1.0448797462514947,1.0448678782128635,1.044856026071015,1.0448441898046574,1.044832369392525,1.044820564813383,1.0448087760460243,1.0447970030692675,1.044785245861963,1.044773504402988,1.0447617786712484,1.0447500686456777,1.0447383743052385,1.0447266956289205,1.044715032595744,1.0447033851847531,1.044691753375024,1.0446801371456593,1.0446685364757893,1.044656951344573,1.044645381731197,1.0446338276148754,1.0446222889748504,1.0446107657903931,1.0445992580408008,1.0445877657053988,1.0445762887635397,1.0445648271946057,1.0445533809780043,1.0445419500931736,1.044530534519575,1.0445191342367002,1.0445077492240686,1.0444963794612263,1.0444850249277455,1.04447368560323,1.044462361467304,1.044451052499626,1.0444397586798768,1.0444284799877679,1.0444172164030359,1.0444059679054447,1.0443947344747844,1.044383516090877,1.044372312733564,1.0443611243827207,1.044349951018245,1.0443387926200638,1.044327649168129,1.0443165206424228,1.0443054070229512,1.0442943082897478,1.044283224422873,1.0442721554024137,1.0442611012084848,1.0442500618212252,1.0442390372208021,1.0442280273874103,1.044217032301268,1.0442060519426242,1.044195086291751,1.0441841353289476,1.0441731990345398,1.0441622773888792,1.0441513703723462,1.0441404779653447,1.0441296001483054,1.0441187369016853,1.0441078882059704,1.0440970540416672,1.044086234389313,1.0440754292294692,1.0440646385427235]}],                        {"scene":{"xaxis":{"title":{"text":"theta0"}},"yaxis":{"title":{"text":"theta1"}},"zaxis":{"title":{"text":"MSE"}}},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('e21e8d65-01ec-4f62-b10c-4588ffecce71');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
</section>
</section>
<section id="batch-mini-batch-and-stochastic-gradient-descent" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="batch-mini-batch-and-stochastic-gradient-descent"><span class="header-section-number">12.4</span> Batch, Mini-Batch, and Stochastic Gradient Descent</h2>
<p>Formally, the algorithm we derived above is called <strong>batch gradient descent.</strong> For each iteration of the algorithm, the derivative of loss is computed across the entire batch of available data. In other words, we compute the derivative of the loss function across all <span class="math inline">\(n\)</span> datapoints in our dataset. While this update rule works well in theory, it is not practical in all circumstances. For large datasets (with perhaps billions of data points), finding the gradient across all the data is incredibly computationally taxing.</p>
<p><strong>Mini-batch gradient descent</strong> tries to address this issue. In mini-batch descent, only a subset of the data is used to compute an estimate of the gradient. The <strong>batch size</strong> is the number of datapoints that are used to approximate the gradient at each update iteration. For example, we might consider only 10% of the total data at each gradient descent update step. At the next iteration, a different 10% of the data is sampled to perform the following update. Once the entire dataset has been used, the process is repeated. Each complete “pass” through the data is known as a <strong>training epoch</strong>. We perform several training epochs until we are satisfied with the optimization result.</p>
<p>In the extreme case, we might choose a batch size of only 1 datapoint – that is, a single data point is used to estimate the gradient of loss with each update step. This is known as <strong>stochastic gradient descent</strong>.</p>
<p>Batch gradient descent is a deterministic technique – because the entire dataset is used at each update iteration, the algorithm will always advance towards the minimum of the loss surface. In contrast, both mini-batch and stochastic gradient descent involve an element of randomness. Since only a subset of the full data is used to update the guess for <span class="math inline">\(\vec{\theta}\)</span> at each iteration, there’s a chance the algorithm will not progress towards the true minimum of loss with each update. Instead, the algorithm <em>approximates</em> the true gradient using only a smaller subset of the data. Over the longer term, these stochastic techniques typically converge towards the true optimal solution.</p>
<p>The diagrams below represent a “bird’s eye view” of a loss surface from above. Notice that batch gradient descent takes a direct path towards the optimal <span class="math inline">\(\hat{\theta}\)</span>. Stochastic gradient descent, in contrast, “hops around” on its path to the minimum point on the loss surface. This reflects the randomness of the sampling process at each update step.</p>
<p><img src="images/bgd.png" alt="batch gradient descent" width="800"></p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../ols/ols.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../feature_engineering/feature_engineering.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Sklearn and Feature Engineering</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb13" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Gradient Descent</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">  warning: false</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-title: Gradient Descent</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co">    page-layout: full</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co">    theme:</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co">      - cosmo</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co">      - cerulean</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co">    callout-icon: false</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Outcomes</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Describe the conceptual basis for gradient descent</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Compute the gradient descent update on a provided dataset</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>At this point, we're fairly comfortable with fitting a regression model under MSE risk (indeed, we've done it twice now!). We have seen how we can algebraically solve for the minimizing value of the model parameter $\theta$, as well as how we can use linear algebra to determine the optimal parameters geometrically.</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>It's important to remember, however, that the results we've found previously apply to one very specific case: the derivations we performed previously are only relevant to a linear regression model using MSE as the cost function. In reality, we'll be working with a wide range of model types and objective functions, not all of which are as straightforward as the scenario we've discussed previously. This means that we need some more generalizable way of fitting a model to minimize loss. </span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>To do this, we'll introduce the technique of **gradient descent**.</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="fu">## Minimizing a 1D Function</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>Let's shift our focus away from MSE to consider some new, arbitrary cost function. You can think of this function as outputting the empirical risk associated with some parameter <span class="in">`theta`</span>. </span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/arbitrary.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'arbitrary'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'600'</span><span class="kw">&gt;</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>Our goal is to find the input that *minimzes* this arbitrary function. In other words, we want to find the position along the x-axis where the function is at its minimum. In a modeling context, you can imagine attempting to find the value of $\theta$ that results in the lowest possible loss for a model.</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Naive Approach: Guess and Check</span></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>Above, we saw that the minimum is somewhere around 5.3ish. Let's see if we can figure out how to find the exact minimum algorithmically from scratch. One way very slow and terrible way would be to manually guess-and-check. The code below "guesses" several possible minimizing values and checks to see which one gives the lowest value of the function.</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> arbitrary(theta):</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The function we would like to minimize</span></span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (theta<span class="op">**</span><span class="dv">4</span> <span class="op">-</span> <span class="dv">15</span><span class="op">*</span>theta<span class="op">**</span><span class="dv">3</span> <span class="op">+</span> <span class="dv">80</span><span class="op">*</span>theta<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">180</span><span class="op">*</span>theta <span class="op">+</span> <span class="dv">144</span>)<span class="op">/</span><span class="dv">10</span></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_minimize(f, thetas):</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> [f(theta) <span class="cf">for</span> theta <span class="kw">in</span> thetas]  </span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> thetas[np.argmin(y)]</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>guesses <span class="op">=</span> [<span class="fl">5.3</span>, <span class="fl">5.31</span>, <span class="fl">5.32</span>, <span class="fl">5.33</span>, <span class="fl">5.34</span>, <span class="fl">5.35</span>]</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>simple_minimize(arbitrary, guesses)</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>This technique moves slowly: we have to manually specify what values we would like to guess. It is also imprecise – if the true minimizing value happened to lie *between* two of our guesses, we would have no way of identifying it. We want a more rigorous method of identifying the minimizing value.</span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a><span class="fu">### `scipy.optimize.minimize`</span></span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>Another method to minimize this mathematical function is to use the <span class="in">`optimize.minimize`</span> function from the <span class="in">`scipy`</span> library. It takes a function and a starting guess, then, it locates the minimum of the function.</span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a><span class="co"># In the readout below, `x` corresponds to the minimizing value of the function</span></span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>minimize(arbitrary, x0 <span class="op">=</span> <span class="fl">3.5</span>)</span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a><span class="in">`scipy.optimize.minimize`</span> is great. It may also seem a bit magical. How can this one line of code find the minimum of any mathematical function so quickly? </span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>Behind the scenes, <span class="in">`scipy.optimize.minimize`</span> uses a technique called **gradient descent** to compute the minimizing value of a function. In this lecture, we will learn the underlying theory behind gradient descent, then implement it ourselves.</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a><span class="fu">### Finding the Minimum Algorithmically</span></span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>Looking at the function across this domain, it is clear that the function's minimum value occurs around $\theta = 5.3$. Let's pretend for a moment that we *couldn't* see the full view of the cost function. How would we guess the value of $\theta$ that minimizes the function? </span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a>It turns out that the first derivative of the function can give us a clue. In the plots below, the line indicates the value of the function's derivative at each value of $\theta$. The derivative is negative where it is red and positive where it is green.</span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a>Say we make a guess for the minimizing value of $\theta$. Remember that we read plots from left to right, and assume that our starting $\theta$ value is to the left of the optimal $\hat{\theta}$. If the guess "undershoots" the true minimizing value – our guess for $\theta$ is not quite at the value of the $\hat{\theta}$ that truly minimizes the function – the derivative will be **negative** in value. This means that if we increase $\theta$ (move further to the right), then we **can decrease** our loss function further. If this guess "overshoots" the true minimizing value, the derivative will be positive in value, implying the converse.</span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/step.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'step'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'600'</span><span class="kw">&gt;</span></span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a>We can use this pattern to help formulate our next guess for the optimal $\hat{\theta}$. Consider the case where we've undershot $\theta$ by guessing too low of a value. We'll want our next guess to be greater in value than the previous guess – that is, we want to shift our guess to the right. You can think of this as following the slope "downhill" to the function's minimum value.</span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/neg_step.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'neg_step'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'600'</span><span class="kw">&gt;</span></span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a>If we've overshot $\hat{\theta}$ by guessing too high of a value, we'll want our next guess to be lower in value – we want to shift our guess for $\hat{\theta}$ to the left. Again, we follow the slope of the curve downhill towards the minimum value.</span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/pos_step.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'pos_step'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'600'</span><span class="kw">&gt;</span></span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a><span class="fu">## Gradient Descent on a 1D Model</span></span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-100"><a href="#cb13-100" aria-hidden="true" tabindex="-1"></a>These observations lead us to the **gradient descent update rule**:</span>
<span id="cb13-101"><a href="#cb13-101" aria-hidden="true" tabindex="-1"></a>$$\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{d}{d\theta}L(\theta^{(t)})$$</span>
<span id="cb13-102"><a href="#cb13-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-103"><a href="#cb13-103" aria-hidden="true" tabindex="-1"></a>Begin with our guess for $\hat{\theta}$ at timestep $t$. To find our guess for $\hat{\theta}$ at the next timestep, $t+1$, subtract the objective function's derivative $\frac{d}{d\theta} L(\theta^{(t)})$ scaled by a positive value $\alpha$. We've replaced the generic function $f$ with $L$ to indicate that we are minimizing loss.</span>
<span id="cb13-104"><a href="#cb13-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-105"><a href="#cb13-105" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If our guess $\theta^{(t)}$ is to the left of $\hat{\theta}$ (undershooting), the first derivative will be negative. Subtracting a negative number from $\theta^{(t)}$ will *increase* the value of the next guess, $\theta^{(t+1)}$. The guess will shift to the right.</span>
<span id="cb13-106"><a href="#cb13-106" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If our guess $\theta^{(t)}$ was too high (overshooting $\hat{\theta}$), the first derivative will be positive. Subtracting a positive number from $\theta^{(t)}$ will *decrease* the value of the next guess, $\theta^{(t+1)}$. The guess will shift to the left.</span>
<span id="cb13-107"><a href="#cb13-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-108"><a href="#cb13-108" aria-hidden="true" tabindex="-1"></a>Put together, this captures the same behavior we reasoned through above. We repeatedly update our guess for the optimal $\theta$ until we've completed a set number of updates, or until each additional update iteration does not change the value of $\theta$. In this second case, we say that gradient descent has **converged** on a solution. Our choice of which stopping condition to use will depending on the specific application.</span>
<span id="cb13-109"><a href="#cb13-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-110"><a href="#cb13-110" aria-hidden="true" tabindex="-1"></a>The $\alpha$ term in the update rule is known as the **learning rate**. It is a positive value represents the size of each gradient descent update step – in other words, how "far" should we step to the left or right with each updated guess? A high value of $\alpha$ will lead to large differences in value between consecutive guesses for $\hat{\theta}$; a low value of $\alpha$ will result in smaller differences in value between consecutive guesses. This is our first example of a **hyperparameter** – a parameter hand-picked by the data scientist that changes the model's behavior – in this course.</span>
<span id="cb13-111"><a href="#cb13-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-112"><a href="#cb13-112" aria-hidden="true" tabindex="-1"></a>If we run gradient descent on our arbitrary function, we produce a more precise estimate of the minimizing $\theta$.</span>
<span id="cb13-113"><a href="#cb13-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-116"><a href="#cb13-116" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-117"><a href="#cb13-117" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb13-118"><a href="#cb13-118" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the derivative of the arbitrary function we want to minimize</span></span>
<span id="cb13-119"><a href="#cb13-119" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> derivative_arbitrary(theta):</span>
<span id="cb13-120"><a href="#cb13-120" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="dv">4</span><span class="op">*</span>theta<span class="op">**</span><span class="dv">3</span> <span class="op">-</span> <span class="dv">45</span><span class="op">*</span>theta<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">160</span><span class="op">*</span>theta <span class="op">-</span> <span class="dv">180</span>)<span class="op">/</span><span class="dv">10</span></span>
<span id="cb13-121"><a href="#cb13-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-122"><a href="#cb13-122" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(df, initial_guess, alpha, n):</span>
<span id="cb13-123"><a href="#cb13-123" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Performs n steps of gradient descent on df using learning rate alpha starting</span></span>
<span id="cb13-124"><a href="#cb13-124" aria-hidden="true" tabindex="-1"></a><span class="co">       from initial_guess. Returns a numpy array of all guesses over time."""</span></span>
<span id="cb13-125"><a href="#cb13-125" aria-hidden="true" tabindex="-1"></a>    guesses <span class="op">=</span> [initial_guess]</span>
<span id="cb13-126"><a href="#cb13-126" aria-hidden="true" tabindex="-1"></a>    current_guess <span class="op">=</span> initial_guess</span>
<span id="cb13-127"><a href="#cb13-127" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(guesses) <span class="op">&lt;</span> n:</span>
<span id="cb13-128"><a href="#cb13-128" aria-hidden="true" tabindex="-1"></a>        current_guess <span class="op">=</span> current_guess <span class="op">-</span> alpha <span class="op">*</span> df(current_guess)</span>
<span id="cb13-129"><a href="#cb13-129" aria-hidden="true" tabindex="-1"></a>        guesses.append(current_guess)</span>
<span id="cb13-130"><a href="#cb13-130" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-131"><a href="#cb13-131" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(guesses)</span>
<span id="cb13-132"><a href="#cb13-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-133"><a href="#cb13-133" aria-hidden="true" tabindex="-1"></a>trajectory <span class="op">=</span> gradient_descent(derivative_arbitrary, <span class="dv">4</span>, <span class="fl">0.3</span>, <span class="dv">20</span>)</span>
<span id="cb13-134"><a href="#cb13-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-135"><a href="#cb13-135" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-136"><a href="#cb13-136" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">1</span>, <span class="dv">7</span>)</span>
<span id="cb13-137"><a href="#cb13-137" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, arbitrary(thetas))</span>
<span id="cb13-138"><a href="#cb13-138" aria-hidden="true" tabindex="-1"></a>plt.scatter(trajectory, arbitrary(trajectory), c<span class="op">=</span><span class="st">"white"</span>, edgecolor<span class="op">=</span><span class="st">"firebrick"</span>, label<span class="op">=</span><span class="st">"Previous guesses"</span>)</span>
<span id="cb13-139"><a href="#cb13-139" aria-hidden="true" tabindex="-1"></a>plt.scatter(trajectory[<span class="op">-</span><span class="dv">1</span>], arbitrary(trajectory[<span class="op">-</span><span class="dv">1</span>]), c<span class="op">=</span><span class="st">"firebrick"</span>, label<span class="op">=</span><span class="st">"Final guess"</span>)</span>
<span id="cb13-140"><a href="#cb13-140" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta$"</span>)</span>
<span id="cb13-141"><a href="#cb13-141" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Cost"</span>)</span>
<span id="cb13-142"><a href="#cb13-142" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span>
<span id="cb13-143"><a href="#cb13-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-144"><a href="#cb13-144" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Minimizing theta: </span><span class="sc">{</span>trajectory[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-145"><a href="#cb13-145" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-146"><a href="#cb13-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-147"><a href="#cb13-147" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convexity</span></span>
<span id="cb13-148"><a href="#cb13-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-149"><a href="#cb13-149" aria-hidden="true" tabindex="-1"></a>In our analysis above, we focused our attention on the global minimum of the loss function. You may be wondering: what about the local minimum just to the left? </span>
<span id="cb13-150"><a href="#cb13-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-151"><a href="#cb13-151" aria-hidden="true" tabindex="-1"></a>If we had chosen a different starting guess for $\theta$, or a different value for the learning rate $\alpha$, we may have converged on the local minimum, rather than on the true optimum value of loss. </span>
<span id="cb13-152"><a href="#cb13-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-155"><a href="#cb13-155" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-156"><a href="#cb13-156" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb13-157"><a href="#cb13-157" aria-hidden="true" tabindex="-1"></a>local_min_trajectory <span class="op">=</span> gradient_descent(derivative_arbitrary, <span class="fl">1.6</span>, <span class="fl">0.75</span>, <span class="dv">20</span>)</span>
<span id="cb13-158"><a href="#cb13-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-159"><a href="#cb13-159" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, arbitrary(thetas))</span>
<span id="cb13-160"><a href="#cb13-160" aria-hidden="true" tabindex="-1"></a>plt.scatter(local_min_trajectory, arbitrary(local_min_trajectory), c<span class="op">=</span><span class="st">"white"</span>, edgecolor<span class="op">=</span><span class="st">"firebrick"</span>, label<span class="op">=</span><span class="st">"Previous guesses"</span>)</span>
<span id="cb13-161"><a href="#cb13-161" aria-hidden="true" tabindex="-1"></a>plt.scatter(local_min_trajectory[<span class="op">-</span><span class="dv">1</span>], arbitrary(local_min_trajectory[<span class="op">-</span><span class="dv">1</span>]), c<span class="op">=</span><span class="st">"firebrick"</span>, label<span class="op">=</span><span class="st">"Final guess"</span>)</span>
<span id="cb13-162"><a href="#cb13-162" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta$"</span>)</span>
<span id="cb13-163"><a href="#cb13-163" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Cost"</span>)</span>
<span id="cb13-164"><a href="#cb13-164" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span>
<span id="cb13-165"><a href="#cb13-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-166"><a href="#cb13-166" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Minimizing theta: </span><span class="sc">{</span>local_min_trajectory[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-167"><a href="#cb13-167" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-168"><a href="#cb13-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-169"><a href="#cb13-169" aria-hidden="true" tabindex="-1"></a>The arbitrary function we have been working with is non-convex: it contains both local and global minima.</span>
<span id="cb13-170"><a href="#cb13-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-171"><a href="#cb13-171" aria-hidden="true" tabindex="-1"></a>Conversely, if a loss function is **convex**, gradient descent is guaranteed to find the global minimum of the objective function. Formally, a function $f$ is convex if:</span>
<span id="cb13-172"><a href="#cb13-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-173"><a href="#cb13-173" aria-hidden="true" tabindex="-1"></a>$$tf(a) + (1-t)f(b) \geq f(ta + (1-t)b)$$</span>
<span id="cb13-174"><a href="#cb13-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-175"><a href="#cb13-175" aria-hidden="true" tabindex="-1"></a>To put this into words: if you drew a line between any two points on the curve, all values on the curve must be *on or below* the line. Importantly, any local minimum of a convex function is also its global minimum. </span>
<span id="cb13-176"><a href="#cb13-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-177"><a href="#cb13-177" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/convex.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'convex'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'600'</span><span class="kw">&gt;</span></span>
<span id="cb13-178"><a href="#cb13-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-179"><a href="#cb13-179" aria-hidden="true" tabindex="-1"></a>Why does this matter? Non-convex loss functions can cause problems with optimization. This means that our choice of loss function is an key factor in our modeling process. It turns out that MSE *is* convex, which is a major reason why it is such a popular choice of loss function.</span>
<span id="cb13-180"><a href="#cb13-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-181"><a href="#cb13-181" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multi-Dimensional Gradient Descent</span></span>
<span id="cb13-182"><a href="#cb13-182" aria-hidden="true" tabindex="-1"></a>We're in good shape now: we've developed a technique to find the minimum value of a more complex objective function. </span>
<span id="cb13-183"><a href="#cb13-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-184"><a href="#cb13-184" aria-hidden="true" tabindex="-1"></a>The function we worked with above was one-dimensional – we were only minimizing the function with respect to a single parameter, $\theta$. However, as we've seen before, we often need to optimize a cost function with respect to several parameters (for example, when selecting the best model parameters for multiple linear regression). We'll need to extend our gradient descent rule to *multi-dimensional* objective functions.</span>
<span id="cb13-185"><a href="#cb13-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-186"><a href="#cb13-186" aria-hidden="true" tabindex="-1"></a>Let's consider our familiar <span class="in">`tips`</span> dataset. </span>
<span id="cb13-187"><a href="#cb13-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-190"><a href="#cb13-190" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-191"><a href="#cb13-191" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb13-192"><a href="#cb13-192" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb13-193"><a href="#cb13-193" aria-hidden="true" tabindex="-1"></a>tips <span class="op">=</span> sns.load_dataset(<span class="st">"tips"</span>)</span>
<span id="cb13-194"><a href="#cb13-194" aria-hidden="true" tabindex="-1"></a>tips.head()</span>
<span id="cb13-195"><a href="#cb13-195" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-196"><a href="#cb13-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-197"><a href="#cb13-197" aria-hidden="true" tabindex="-1"></a><span class="fu">### Loss Surfaces </span></span>
<span id="cb13-198"><a href="#cb13-198" aria-hidden="true" tabindex="-1"></a>Suppose we want to apply simple linear regression to predict the <span class="in">`"tip"`</span> from the <span class="in">`"total_bill"`</span> and an intercept term. Our model takes the form:</span>
<span id="cb13-199"><a href="#cb13-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-200"><a href="#cb13-200" aria-hidden="true" tabindex="-1"></a>$$\hat{y} = \theta_0 + \theta_1 x$$</span>
<span id="cb13-201"><a href="#cb13-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-202"><a href="#cb13-202" aria-hidden="true" tabindex="-1"></a>We'll use the MSE loss function to quantify our model's error.</span>
<span id="cb13-203"><a href="#cb13-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-204"><a href="#cb13-204" aria-hidden="true" tabindex="-1"></a>$$\text{MSE}(\theta_0,\:\theta_1) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - \theta_0 - \theta_1 x)^2$$</span>
<span id="cb13-205"><a href="#cb13-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-206"><a href="#cb13-206" aria-hidden="true" tabindex="-1"></a>Notice that our model contains two parameters, $\theta_0$ and $\theta_1$. This means that we now need to optimize *two* different parameters to determine their optimal values. Rather than a one-dimensional loss function like we saw in the previous section, we are now dealing with a **loss surface**. </span>
<span id="cb13-207"><a href="#cb13-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-208"><a href="#cb13-208" aria-hidden="true" tabindex="-1"></a>A loss surface visualizes how the model's loss changes with different *combinations* of parameter values. Each point on the surface tells us what the model's loss will be for a given choice of $\theta_0$ and $\theta_1$. Click and drag on the visualization below to explore the surface. We have marked the optimal choice of model parameters in red.</span>
<span id="cb13-209"><a href="#cb13-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-212"><a href="#cb13-212" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-213"><a href="#cb13-213" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb13-214"><a href="#cb13-214" aria-hidden="true" tabindex="-1"></a><span class="co"># This code is for illustration purposes only</span></span>
<span id="cb13-215"><a href="#cb13-215" aria-hidden="true" tabindex="-1"></a><span class="co"># It contains a lot of syntax you have not seen before</span></span>
<span id="cb13-216"><a href="#cb13-216" aria-hidden="true" tabindex="-1"></a><span class="co"># For now, just focus on the outputted plot</span></span>
<span id="cb13-217"><a href="#cb13-217" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb13-218"><a href="#cb13-218" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.linear_model <span class="im">as</span> lm</span>
<span id="cb13-219"><a href="#cb13-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-220"><a href="#cb13-220" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> lm.LinearRegression(fit_intercept <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb13-221"><a href="#cb13-221" aria-hidden="true" tabindex="-1"></a>tips[<span class="st">"bias"</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb13-222"><a href="#cb13-222" aria-hidden="true" tabindex="-1"></a>model.fit(tips[[<span class="st">"bias"</span>,<span class="st">"total_bill"</span>]], tips[<span class="st">"tip"</span>])</span>
<span id="cb13-223"><a href="#cb13-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-224"><a href="#cb13-224" aria-hidden="true" tabindex="-1"></a>uvalues <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">10</span>)</span>
<span id="cb13-225"><a href="#cb13-225" aria-hidden="true" tabindex="-1"></a>vvalues <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="dv">10</span>)</span>
<span id="cb13-226"><a href="#cb13-226" aria-hidden="true" tabindex="-1"></a>(u,v) <span class="op">=</span> np.meshgrid(uvalues, vvalues)</span>
<span id="cb13-227"><a href="#cb13-227" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.vstack((u.flatten(),v.flatten()))</span>
<span id="cb13-228"><a href="#cb13-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-229"><a href="#cb13-229" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> tips[[<span class="st">"bias"</span>,<span class="st">"total_bill"</span>]].to_numpy()</span>
<span id="cb13-230"><a href="#cb13-230" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> tips[<span class="st">"tip"</span>].to_numpy()</span>
<span id="cb13-231"><a href="#cb13-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-232"><a href="#cb13-232" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_loss_single_arg(theta):</span>
<span id="cb13-233"><a href="#cb13-233" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mse_loss(theta, X, Y)</span>
<span id="cb13-234"><a href="#cb13-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-235"><a href="#cb13-235" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_loss(theta, X, y_obs):</span>
<span id="cb13-236"><a href="#cb13-236" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> X <span class="op">@</span> theta</span>
<span id="cb13-237"><a href="#cb13-237" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean((y_hat <span class="op">-</span> Y) <span class="op">**</span> <span class="dv">2</span>)    </span>
<span id="cb13-238"><a href="#cb13-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-239"><a href="#cb13-239" aria-hidden="true" tabindex="-1"></a>MSE <span class="op">=</span> np.array([mse_loss_single_arg(t) <span class="cf">for</span> t <span class="kw">in</span> thetas.T])</span>
<span id="cb13-240"><a href="#cb13-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-241"><a href="#cb13-241" aria-hidden="true" tabindex="-1"></a>loss_surface <span class="op">=</span> go.Surface(x<span class="op">=</span>u, y<span class="op">=</span>v, z<span class="op">=</span>np.reshape(MSE, u.shape))</span>
<span id="cb13-242"><a href="#cb13-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-243"><a href="#cb13-243" aria-hidden="true" tabindex="-1"></a>ind <span class="op">=</span> np.argmin(MSE)</span>
<span id="cb13-244"><a href="#cb13-244" aria-hidden="true" tabindex="-1"></a>optimal_point <span class="op">=</span> go.Scatter3d(name <span class="op">=</span> <span class="st">"Optimal Point"</span>,</span>
<span id="cb13-245"><a href="#cb13-245" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> [thetas.T[ind,<span class="dv">0</span>]], y <span class="op">=</span> [thetas.T[ind,<span class="dv">1</span>]], </span>
<span id="cb13-246"><a href="#cb13-246" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> [MSE[ind]],</span>
<span id="cb13-247"><a href="#cb13-247" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">"red"</span>))</span>
<span id="cb13-248"><a href="#cb13-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-249"><a href="#cb13-249" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> go.Figure(data<span class="op">=</span>[loss_surface, optimal_point])</span>
<span id="cb13-250"><a href="#cb13-250" aria-hidden="true" tabindex="-1"></a>fig.update_layout(scene <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb13-251"><a href="#cb13-251" aria-hidden="true" tabindex="-1"></a>    xaxis_title <span class="op">=</span> <span class="st">"theta0"</span>,</span>
<span id="cb13-252"><a href="#cb13-252" aria-hidden="true" tabindex="-1"></a>    yaxis_title <span class="op">=</span> <span class="st">"theta1"</span>,</span>
<span id="cb13-253"><a href="#cb13-253" aria-hidden="true" tabindex="-1"></a>    zaxis_title <span class="op">=</span> <span class="st">"MSE"</span>))</span>
<span id="cb13-254"><a href="#cb13-254" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb13-255"><a href="#cb13-255" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-256"><a href="#cb13-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-257"><a href="#cb13-257" aria-hidden="true" tabindex="-1"></a>It is often easier to visualize a loss surface by using a contour plot. You can imagine the visualization below as being a "bird's eye view" of the 2D loss surface from above.</span>
<span id="cb13-258"><a href="#cb13-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-261"><a href="#cb13-261" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-262"><a href="#cb13-262" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb13-263"><a href="#cb13-263" aria-hidden="true" tabindex="-1"></a>loss_contour <span class="op">=</span> go.Contour(x<span class="op">=</span>u[<span class="dv">0</span>], y<span class="op">=</span>v[:, <span class="dv">0</span>], z<span class="op">=</span>np.reshape(MSE, u.shape))</span>
<span id="cb13-264"><a href="#cb13-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-265"><a href="#cb13-265" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> go.Figure(data<span class="op">=</span>[loss_contour])</span>
<span id="cb13-266"><a href="#cb13-266" aria-hidden="true" tabindex="-1"></a>fig.update_layout(scene <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb13-267"><a href="#cb13-267" aria-hidden="true" tabindex="-1"></a>    xaxis_title <span class="op">=</span> <span class="st">"theta0"</span>,</span>
<span id="cb13-268"><a href="#cb13-268" aria-hidden="true" tabindex="-1"></a>    yaxis_title <span class="op">=</span> <span class="st">"theta1"</span>,</span>
<span id="cb13-269"><a href="#cb13-269" aria-hidden="true" tabindex="-1"></a>    zaxis_title <span class="op">=</span> <span class="st">"MSE"</span>))</span>
<span id="cb13-270"><a href="#cb13-270" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb13-271"><a href="#cb13-271" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-272"><a href="#cb13-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-273"><a href="#cb13-273" aria-hidden="true" tabindex="-1"></a><span class="fu">### Partial Derivatives</span></span>
<span id="cb13-274"><a href="#cb13-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-275"><a href="#cb13-275" aria-hidden="true" tabindex="-1"></a>Though our objective function looks a little different, we can use the same principles as we did earlier to locate the optimal model parameters. Notice how the minimum value of MSE, marked by the red dot in the plot above, occurs in the "valley" of the loss surface. Like before, we want our guesses for the best pair of $(\theta_0,\:\theta_1)$ to move "downhill" towards this minimum point. </span>
<span id="cb13-276"><a href="#cb13-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-277"><a href="#cb13-277" aria-hidden="true" tabindex="-1"></a>With a small adjustment, we can use the same gradient descent algorithm we derived earlier. Our loss function is now a **multivariable function**. It depends on two variables, $\theta_0$ and $\theta_1$. We'll need to take the derivative with respect to one of these variables at a time. </span>
<span id="cb13-278"><a href="#cb13-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-279"><a href="#cb13-279" aria-hidden="true" tabindex="-1"></a>A **partial derivative** is the derivative of a multivariable function with respect to just *one* variable. We treat all other variables in the function as constants, and instead focus our attention on just the variable of interest. For a multivariable function $f(x, y)$, the symbol $\frac{\partial f} {\partial x}$ denotes the partial derivative of $f$ with respect to $x$ (holding $y$ constant), while the symbol $\frac{\partial f} {\partial y}$ denotes the partial derivative of $f$ with respect to $y$ (holding $x$ constant).</span>
<span id="cb13-280"><a href="#cb13-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-281"><a href="#cb13-281" aria-hidden="true" tabindex="-1"></a>$$f(x, y) = 3x^2 + y$$</span>
<span id="cb13-282"><a href="#cb13-282" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial f} {\partial x} = 6x\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\frac{\partial f} {\partial y} = 1$$  </span>
<span id="cb13-283"><a href="#cb13-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-284"><a href="#cb13-284" aria-hidden="true" tabindex="-1"></a>We can interpret a partial derivative as saying "if change one variable while holding all other variables constant, how will the function change?"</span>
<span id="cb13-285"><a href="#cb13-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-286"><a href="#cb13-286" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gradient Descent in Vector Form</span></span>
<span id="cb13-287"><a href="#cb13-287" aria-hidden="true" tabindex="-1"></a>Recall our gradient descent update rule from before:</span>
<span id="cb13-288"><a href="#cb13-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-289"><a href="#cb13-289" aria-hidden="true" tabindex="-1"></a>$$\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{d}{d\theta}L(\theta^{(t)})$$</span>
<span id="cb13-290"><a href="#cb13-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-291"><a href="#cb13-291" aria-hidden="true" tabindex="-1"></a>When optimizing a model with just one parameter, we iteratively updated guesses for only one $\theta$.</span>
<span id="cb13-292"><a href="#cb13-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-293"><a href="#cb13-293" aria-hidden="true" tabindex="-1"></a>In our new model with two parameters, we need to update guesses for *both* $\theta_0$ and $\theta_1$ that minimize our loss function $L(\theta_0, \theta_1)$. This means that we apply our gradient update rule for *each* parameter $\theta_i$. Because the loss function $L(\theta_0, \theta_1)$ is now a function of two variables, we compute its partial derivatives with respect to $\theta_0$ and $\theta_1$.</span>
<span id="cb13-294"><a href="#cb13-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-295"><a href="#cb13-295" aria-hidden="true" tabindex="-1"></a>$$\theta_0^{(t+1)} = \theta_0^{(t)} - \alpha \frac{\partial L}{\partial \theta_0}\Bigm\vert_{\theta=\theta^{(t)}} \qquad \qquad \theta_1^{(t+1)} = \theta_1^{(t)} - \alpha \frac{\partial L}{\partial \theta_1}\Bigm\vert_{\theta=\theta^{(t)}}$$</span>
<span id="cb13-296"><a href="#cb13-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-297"><a href="#cb13-297" aria-hidden="true" tabindex="-1"></a>We can tidy this statement up by using vector notation:</span>
<span id="cb13-298"><a href="#cb13-298" aria-hidden="true" tabindex="-1"></a>$$\begin{bmatrix}</span>
<span id="cb13-299"><a href="#cb13-299" aria-hidden="true" tabindex="-1"></a>           \theta_{0}^{(t+1)} <span class="sc">\\</span></span>
<span id="cb13-300"><a href="#cb13-300" aria-hidden="true" tabindex="-1"></a>           \theta_{1}^{(t+1)} <span class="sc">\\</span></span>
<span id="cb13-301"><a href="#cb13-301" aria-hidden="true" tabindex="-1"></a>         \end{bmatrix}</span>
<span id="cb13-302"><a href="#cb13-302" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb13-303"><a href="#cb13-303" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}</span>
<span id="cb13-304"><a href="#cb13-304" aria-hidden="true" tabindex="-1"></a>           \theta_{0}^{(t)} <span class="sc">\\</span></span>
<span id="cb13-305"><a href="#cb13-305" aria-hidden="true" tabindex="-1"></a>           \theta_{1}^{(t)} <span class="sc">\\</span></span>
<span id="cb13-306"><a href="#cb13-306" aria-hidden="true" tabindex="-1"></a>         \end{bmatrix}</span>
<span id="cb13-307"><a href="#cb13-307" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>\alpha</span>
<span id="cb13-308"><a href="#cb13-308" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}</span>
<span id="cb13-309"><a href="#cb13-309" aria-hidden="true" tabindex="-1"></a>           \frac{\partial L}{\partial \theta_{0}}\vert_{\theta=\theta^{(t)}} <span class="sc">\\</span></span>
<span id="cb13-310"><a href="#cb13-310" aria-hidden="true" tabindex="-1"></a>           \frac{\partial L}{\partial \theta_{1}}\vert_{\theta=\theta^{(t)}} <span class="sc">\\</span></span>
<span id="cb13-311"><a href="#cb13-311" aria-hidden="true" tabindex="-1"></a>         \end{bmatrix}</span>
<span id="cb13-312"><a href="#cb13-312" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-313"><a href="#cb13-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-314"><a href="#cb13-314" aria-hidden="true" tabindex="-1"></a>To save ourselves from writing out long column vectors, we'll introduce some new notation. $\vec{\theta}^{(t)}$ is a column vector of guesses for each model parameter $\theta_i$ at timestep $t$. We call $\nabla_{\vec{\theta}} L$ the **gradient vector.** In plain English, it means "take the derivative of loss, $L$, with respect to each model parameter in $\vec{\theta}$, and evaluate it at the given $\theta = \theta^{(t)}$."</span>
<span id="cb13-315"><a href="#cb13-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-316"><a href="#cb13-316" aria-hidden="true" tabindex="-1"></a>$$\vec{\theta}^{(t+1)}</span>
<span id="cb13-317"><a href="#cb13-317" aria-hidden="true" tabindex="-1"></a>= \vec{\theta}^{(t)} - \alpha \nabla_{\vec{\theta}} L(\theta^{(t)})</span>
<span id="cb13-318"><a href="#cb13-318" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-319"><a href="#cb13-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-320"><a href="#cb13-320" aria-hidden="true" tabindex="-1"></a>Let's see this in action. In the cell below, we compute the gradient vector for our choice of model and loss function, then run the gradient descent algorithm. We see that our final guess lies very close to the true minimum of the loss surface!</span>
<span id="cb13-321"><a href="#cb13-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-324"><a href="#cb13-324" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-325"><a href="#cb13-325" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb13-326"><a href="#cb13-326" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_gradient(theta, X, y_obs):</span>
<span id="cb13-327"><a href="#cb13-327" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the gradient of the MSE on our data for the given theta"""</span>    </span>
<span id="cb13-328"><a href="#cb13-328" aria-hidden="true" tabindex="-1"></a>    x0 <span class="op">=</span> X.iloc[:, <span class="dv">0</span>]</span>
<span id="cb13-329"><a href="#cb13-329" aria-hidden="true" tabindex="-1"></a>    x1 <span class="op">=</span> X.iloc[:, <span class="dv">1</span>]</span>
<span id="cb13-330"><a href="#cb13-330" aria-hidden="true" tabindex="-1"></a>    dth0 <span class="op">=</span> np.mean(<span class="op">-</span><span class="dv">2</span> <span class="op">*</span> (y_obs <span class="op">-</span> theta[<span class="dv">0</span>]<span class="op">*</span>x0 <span class="op">-</span> theta[<span class="dv">1</span>]<span class="op">*</span>x1) <span class="op">*</span> x0)</span>
<span id="cb13-331"><a href="#cb13-331" aria-hidden="true" tabindex="-1"></a>    dth1 <span class="op">=</span> np.mean(<span class="op">-</span><span class="dv">2</span> <span class="op">*</span> (y_obs <span class="op">-</span> theta[<span class="dv">0</span>]<span class="op">*</span>x0 <span class="op">-</span> theta[<span class="dv">1</span>]<span class="op">*</span>x1) <span class="op">*</span> x1)</span>
<span id="cb13-332"><a href="#cb13-332" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array([dth0, dth1])</span>
<span id="cb13-333"><a href="#cb13-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-334"><a href="#cb13-334" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_gradient_single_arg(theta):</span>
<span id="cb13-335"><a href="#cb13-335" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the gradient of the MSE on our data for the given theta"""</span></span>
<span id="cb13-336"><a href="#cb13-336" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> tips[[<span class="st">"bias"</span>, <span class="st">"total_bill"</span>]]</span>
<span id="cb13-337"><a href="#cb13-337" aria-hidden="true" tabindex="-1"></a>    y_obs <span class="op">=</span> tips[<span class="st">"tip"</span>]</span>
<span id="cb13-338"><a href="#cb13-338" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mse_gradient(theta, X, y_obs)</span>
<span id="cb13-339"><a href="#cb13-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-340"><a href="#cb13-340" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_single_arg(theta):</span>
<span id="cb13-341"><a href="#cb13-341" aria-hidden="true" tabindex="-1"></a>    theta_0, theta_1 <span class="op">=</span> theta[<span class="dv">0</span>], theta[<span class="dv">1</span>]</span>
<span id="cb13-342"><a href="#cb13-342" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean((tips[<span class="st">"tip"</span>]<span class="op">-</span>theta_0<span class="op">-</span>theta_1<span class="op">*</span>tips[<span class="st">"total_bill"</span>])<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-343"><a href="#cb13-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-344"><a href="#cb13-344" aria-hidden="true" tabindex="-1"></a>guesses <span class="op">=</span> gradient_descent(mse_gradient_single_arg, np.array([<span class="fl">0.5</span>, <span class="fl">0.3</span>]), <span class="fl">0.002</span>, <span class="dv">1000</span>)</span>
<span id="cb13-345"><a href="#cb13-345" aria-hidden="true" tabindex="-1"></a>mses <span class="op">=</span> [mse_single_arg(theta) <span class="cf">for</span> theta <span class="kw">in</span> guesses]</span>
<span id="cb13-346"><a href="#cb13-346" aria-hidden="true" tabindex="-1"></a>trajectory <span class="op">=</span> go.Scatter3d(x<span class="op">=</span>guesses[:, <span class="dv">0</span>], y<span class="op">=</span>guesses[:, <span class="dv">1</span>], z<span class="op">=</span>mses, marker<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">3</span>))</span>
<span id="cb13-347"><a href="#cb13-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-348"><a href="#cb13-348" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> go.Figure(data<span class="op">=</span>[loss_surface, trajectory])</span>
<span id="cb13-349"><a href="#cb13-349" aria-hidden="true" tabindex="-1"></a>fig.update_layout(scene <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb13-350"><a href="#cb13-350" aria-hidden="true" tabindex="-1"></a>    xaxis_title <span class="op">=</span> <span class="st">"theta0"</span>,</span>
<span id="cb13-351"><a href="#cb13-351" aria-hidden="true" tabindex="-1"></a>    yaxis_title <span class="op">=</span> <span class="st">"theta1"</span>,</span>
<span id="cb13-352"><a href="#cb13-352" aria-hidden="true" tabindex="-1"></a>    zaxis_title <span class="op">=</span> <span class="st">"MSE"</span>))</span>
<span id="cb13-353"><a href="#cb13-353" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb13-354"><a href="#cb13-354" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-355"><a href="#cb13-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-356"><a href="#cb13-356" aria-hidden="true" tabindex="-1"></a><span class="fu">## Batch, Mini-Batch, and Stochastic Gradient Descent</span></span>
<span id="cb13-357"><a href="#cb13-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-358"><a href="#cb13-358" aria-hidden="true" tabindex="-1"></a>Formally, the algorithm we derived above is called **batch gradient descent.** For each iteration of the algorithm, the derivative of loss is computed across the entire batch of available data. In other words, we compute the derivative of the loss function across all $n$ datapoints in our dataset. While this update rule works well in theory, it is not practical in all circumstances. For large datasets (with perhaps billions of data points), finding the gradient across all the data is incredibly computationally taxing. </span>
<span id="cb13-359"><a href="#cb13-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-360"><a href="#cb13-360" aria-hidden="true" tabindex="-1"></a>**Mini-batch gradient descent** tries to address this issue. In mini-batch descent, only a subset of the data is used to compute an estimate of the gradient. The **batch size** is the number of datapoints that are used to approximate the gradient at each update iteration. For example, we might consider only 10% of the total data at each gradient descent update step. At the next iteration, a different 10% of the data is sampled to perform the following update. Once the entire dataset has been used, the process is repeated. Each complete "pass" through the data is known as a **training epoch**. We perform several training epochs until we are satisfied with the optimization result. </span>
<span id="cb13-361"><a href="#cb13-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-362"><a href="#cb13-362" aria-hidden="true" tabindex="-1"></a>In the extreme case, we might choose a batch size of only 1 datapoint – that is, a single data point is used to estimate the gradient of loss with each update step. This is known as **stochastic gradient descent**.</span>
<span id="cb13-363"><a href="#cb13-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-364"><a href="#cb13-364" aria-hidden="true" tabindex="-1"></a>Batch gradient descent is a deterministic technique – because the entire dataset is used at each update iteration, the algorithm will always advance towards the minimum of the loss surface. In contrast, both mini-batch and stochastic gradient descent involve an element of randomness. Since only a subset of the full data is used to update the guess for $\vec{\theta}$ at each iteration, there's a chance the algorithm will not progress towards the true minimum of loss with each update. Instead, the algorithm *approximates* the true gradient using only a smaller subset of the data. Over the longer term, these stochastic techniques typically converge towards the true optimal solution. </span>
<span id="cb13-365"><a href="#cb13-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-366"><a href="#cb13-366" aria-hidden="true" tabindex="-1"></a>The diagrams below represent a "bird's eye view" of a loss surface from above. Notice that batch gradient descent takes a direct path towards the optimal $\hat{\theta}$. Stochastic gradient descent, in contrast, "hops around" on its path to the minimum point on the loss surface. This reflects the randomness of the sampling process at each update step.</span>
<span id="cb13-367"><a href="#cb13-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-368"><a href="#cb13-368" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/bgd.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'batch gradient descent'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'800'</span><span class="kw">&gt;</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>