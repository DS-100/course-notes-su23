{
  "hash": "052690e2d5642b1b8afab3af0406c2bc",
  "result": {
    "markdown": "---\ntitle: Visualization\nformat:\n  html:\n    toc: true\n    toc-depth: 5\n    toc-location: right\n    code-fold: false\n    theme:\n      - cosmo\n      - cerulean\n    callout-icon: false\n---\n\n::: {.callout-note collapse=\"true\"}\n## Learning Outcomes\n- Visualize distributions and relationships using the Matplotlib and Seaborn plotting libraries\n- Apply kernel density estimation to smooth data\n- Transform data in preparation for visualization and modeling\n:::\n\nIn our journey of the data science lifecycle, we have begun to explore the vast world of exploratory data analysis. More recently, we learned how to pre-process data using various manipulation and cleaning techniques. As we work towards understanding our data, there is one key component missing in our arsenal - the ability to visualize and discern relationships in existing data.\n\nThis lecture will introduce you to various examples of data visualizations and their underlying theory. In doing so, we'll motivate their importance in real-world examples with the use of plotting libraries.\n\n## Visualizations in Data 8 (and Data 100, so far)\n\nYou've likely encountered several forms of data visualizations in your studies. You may remember two such examples from Data 8: line charts and histograms. Each of these served a unique purpose. For example, line charts displayed how numerical quantities changed over time, while histograms were useful in understanding a variable's distribution. \n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n**Line Chart**\n\n<img src=\"images/line_chart_viz.png\" alt='line_chart_viz' width='300'>\n:::\n\n::: {.column width=\"30%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Histogram**\n\n<img src=\"images/histogram_viz.png\" alt='histogram_viz' width='300'>\n:::\n\n::::\n\n## Goals of Visualization\n\nVisualizations are useful for a number of reasons. In Data 100, we consider two areas in particular:\n\n1. To help your own understanding of the data\n    - Key part of exploratory data analysis.\n    - Summarize trends visually before in-depth analysis.\n2. To communicate results/conclusions to others \n    - Fine-tuned to achieve a communications goal.\n    - Considerations: clarity, accessibility, providing necessary context.\n    \nPut together, these goals add more nuance to our visualization task. Visualization is not simply a matter of making \"pretty\" pictures; rather, we will need to do a lot of thinking about what stylistic choices communicate ideas in the most effective way.\n\nOne of the most common applications of visualizations is in understanding a distribution of data.\n\n## Visualizing Distributions\n\n### Distributions\nA distribution describes the frequency of unique values in a variable. Distributions must satisfy two properties:\n\n   1. Each data point must belong to only one category.\n   2. The total frequency of all categories must sum to 100%. In other words, their total count should equal the number of values in consideration.\n   \n:::: {.columns}\n\n::: {.column width=\"30%\"}\n**Not a Valid Distribution**\n\n<img src=\"images/bad_distro.png\" alt='bad_distro' width='300'>\n:::\n\n::: {.column width=\"30%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Valid Distribution**\n\n<img src=\"images/good_distro.png\" alt='good_distro' width='300'>\n:::\n\n::::\n\nLeft Diagram: This is not a valid distribution since individuals can be associated to more than one category. The bar values indicate a *change* in time, rather than the count or proportion of individuals belonging to each categoy. \n\nRight Diagram: This example satisfies the two properties of distributions, so it is a valid distribution. Each bar represents the percentage of individuals who belonging to the corresponding category. All percentages sum to 100%, as required.\n\n### Variable Types\nIn our previous work on EDA, we learned how to categorize variables by their type. We saw that some variables are quantitative while others are qualitative; we also discussed the sub-divisions of variables into continuous, discrete, ordinal, and nominal.\n\n<img src=\"images/variable.png\" alt='variable' width='500'>\n\nDifferent plot types are more or less suited for displaying particular types of variables. Before creating a visualization, you should always ask yourself: what types of variables am I presenting? What plots are suitable to present this variable type?\n\nA summary of the plots generally used to display different variable types is given below. In the following sections, we will introduce each type of plot and learn how to generate them in code.\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n**Distribution of a qualitative variable:**\n\n* Bar plot\n<br>\n<br>\n<br>\n<br>\n<br>\n\n**Distribution of a continuous variable across qualitative categories:**\n\n* Overlaid histograms\n* Side-by-side box plots\n* Side-by-side violin plots\n* Overlaid KDE plots\n:::\n\n::: {.column width=\"10%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"40%\"}\n**Distribution of a quantitative continuous variable:**\n\n* Histogram\n* Box plot\n* Violin plot\n* KDE plot\n\n**Relationship between quantitative continuous variables:**\n\n* Scatter plot\n* Hex plot\n* Contour map\n:::\n\n::::\n\n### Bar Plots\n\nAs we saw above, a **bar plot** is one of the most common ways of displaying the distribution of a **qualitative** (categorical) variable. The length of a bar plot encodes the frequency of a category; the width encodes no useful information.\n\nLet's contextualize this in an example. We will use the World Bank dataset `wb`, which contains information about countries and social statistics.\n\n::: {.cell tags='[]' execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport pandas as pd\n\nwb = pd.read_csv(\"data/world_bank.csv\", index_col=0)\nwb.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Continent</th>\n      <th>Country</th>\n      <th>Primary completion rate: Male: % of relevant age group: 2015</th>\n      <th>Primary completion rate: Female: % of relevant age group: 2015</th>\n      <th>Lower secondary completion rate: Male: % of relevant age group: 2015</th>\n      <th>Lower secondary completion rate: Female: % of relevant age group: 2015</th>\n      <th>Youth literacy rate: Male: % of ages 15-24: 2005-14</th>\n      <th>Youth literacy rate: Female: % of ages 15-24: 2005-14</th>\n      <th>Adult literacy rate: Male: % ages 15 and older: 2005-14</th>\n      <th>Adult literacy rate: Female: % ages 15 and older: 2005-14</th>\n      <th>...</th>\n      <th>Access to improved sanitation facilities: % of population: 1990</th>\n      <th>Access to improved sanitation facilities: % of population: 2015</th>\n      <th>Child immunization rate: Measles: % of children ages 12-23 months: 2015</th>\n      <th>Child immunization rate: DTP3: % of children ages 12-23 months: 2015</th>\n      <th>Children with acute respiratory infection taken to health provider: % of children under age 5 with ARI: 2009-2016</th>\n      <th>Children with diarrhea who received oral rehydration and continuous feeding: % of children under age 5 with diarrhea: 2009-2016</th>\n      <th>Children sleeping under treated bed nets: % of children under age 5: 2009-2016</th>\n      <th>Children with fever receiving antimalarial drugs: % of children under age 5 with fever: 2009-2016</th>\n      <th>Tuberculosis: Treatment success rate: % of new cases: 2014</th>\n      <th>Tuberculosis: Cases detection rate: % of new estimated cases: 2015</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Africa</td>\n      <td>Algeria</td>\n      <td>106.0</td>\n      <td>105.0</td>\n      <td>68.0</td>\n      <td>85.0</td>\n      <td>96.0</td>\n      <td>92.0</td>\n      <td>83.0</td>\n      <td>68.0</td>\n      <td>...</td>\n      <td>80.0</td>\n      <td>88.0</td>\n      <td>95.0</td>\n      <td>95.0</td>\n      <td>66.0</td>\n      <td>42.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>88.0</td>\n      <td>80.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Africa</td>\n      <td>Angola</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>79.0</td>\n      <td>67.0</td>\n      <td>82.0</td>\n      <td>60.0</td>\n      <td>...</td>\n      <td>22.0</td>\n      <td>52.0</td>\n      <td>55.0</td>\n      <td>64.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>25.9</td>\n      <td>28.3</td>\n      <td>34.0</td>\n      <td>64.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Africa</td>\n      <td>Benin</td>\n      <td>83.0</td>\n      <td>73.0</td>\n      <td>50.0</td>\n      <td>37.0</td>\n      <td>55.0</td>\n      <td>31.0</td>\n      <td>41.0</td>\n      <td>18.0</td>\n      <td>...</td>\n      <td>7.0</td>\n      <td>20.0</td>\n      <td>75.0</td>\n      <td>79.0</td>\n      <td>23.0</td>\n      <td>33.0</td>\n      <td>72.7</td>\n      <td>25.9</td>\n      <td>89.0</td>\n      <td>61.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Africa</td>\n      <td>Botswana</td>\n      <td>98.0</td>\n      <td>101.0</td>\n      <td>86.0</td>\n      <td>87.0</td>\n      <td>96.0</td>\n      <td>99.0</td>\n      <td>87.0</td>\n      <td>89.0</td>\n      <td>...</td>\n      <td>39.0</td>\n      <td>63.0</td>\n      <td>97.0</td>\n      <td>95.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>77.0</td>\n      <td>62.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Africa</td>\n      <td>Burundi</td>\n      <td>58.0</td>\n      <td>66.0</td>\n      <td>35.0</td>\n      <td>30.0</td>\n      <td>90.0</td>\n      <td>88.0</td>\n      <td>89.0</td>\n      <td>85.0</td>\n      <td>...</td>\n      <td>42.0</td>\n      <td>48.0</td>\n      <td>93.0</td>\n      <td>94.0</td>\n      <td>55.0</td>\n      <td>43.0</td>\n      <td>53.8</td>\n      <td>25.4</td>\n      <td>91.0</td>\n      <td>51.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 47 columns</p>\n</div>\n```\n:::\n:::\n\n\nWe can visualize the distribution of the `\"Continent\"` column using a bar plot. There are a few ways to do this.\n\n#### Plotting in Matplotlib\n\n[Matplotlib](https://matplotlib.org/stable/api/index) is a common library used for generating visualizations in Python. Most Matplotlib plotting functions follow the same structure: we pass in a sequence (list, array, or Series) of values to be plotted on the x-axis, and a second sequence of values to be plotted on the y-axis. In pseudocode, Matplotlib plotting calls typically have the form:\n    \n    # plt is the typical alias for Matplotlib\n    import matplotlib.pyplot as plt \n    plt.plotting_function(x_values, y_values)\n\nTo add labels and a title to a plot, we call the functions `plt.xlabel`, `plt.ylabel`, and `plt.title`.\n\n    plt.xlabel(“x axis label”)\n    plt.ylabel(“y axis label”)\n    plt.title(“Title of the plot”);\n\nThe plotting function to create a bar plot in Matplotlib is `plt.bar`. To generate a bar plot, we will need to create a set of x values and y values to pass in as arguments to `plt.bar`. In the cell below, we call `.value_counts()` to obtain the distribution of the `\"Continent\"` column. The index of the resulting `Series` contains our desired x values (the names of each unique continent), while the values contain our y data (the counts of countries in each continent).\n\n::: {.cell tags='[]' execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\ncontinents = wb[\"Continent\"].value_counts()\nplt.bar(continents.index, continents)\n\nplt.xlabel(\"Continent\")\nplt.ylabel(\"Count of countries\")\nplt.title(\"Distribution of countries across continents\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-3-output-1.png){width=585 height=449}\n:::\n:::\n\n\nYou may have noticed the semicolon `;` at the end of cell above. This suppresses any unnecessary output other than the plot. If we do not include a semicolon, the plot will still generate, however, we will see extraneous text as well:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nplt.bar(continents.index, continents)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n<BarContainer object of 6 artists>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-4-output-2.png){width=566 height=411}\n:::\n:::\n\n\n#### Plotting in Seaborn\n\n[Seaborn](https://seaborn.pydata.org/) is another popular Python library for creating plots. In Data 100, we will explore the use of both Matplotlib and Seaborn. You may find that you prefer one over the other – if so, you are welcome to use your favorite method! We present plotting code for both libraries to help familiarize you with their use. \n\nSeaborn plotting functions use a different structure to what we saw in Matplotlib. In Seaborn, we pass in an entire `DataFrame` as an argument, then specify what column(s) from the `DataFrame` to plot. In pseudocode:\n    \n    # sns is the typical alias for Seaborn\n    import seaborn as sns\n    sns.plotting_function(data = df, x = \"x_col\", y = \"y_col\")\n    \nAs before, we can add axis labels and a title:\n\n    plt.xlabel(“x axis label”)\n    plt.ylabel(“y axis label”)\n    plt.title(“Title of the plot”)\n\nThe Seaborn plotting function for creating a bar plot is `sns.countplot`.\n\n::: {.cell tags='[]' execution_count=4}\n``` {.python .cell-code}\nimport seaborn as sns\nsns.countplot(data = wb, x = 'Continent');\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-5-output-1.png){width=585 height=429}\n:::\n:::\n\n\n`sns.countplot` both counts and visualizes the number of unique values in a given column, without the need for us to pre-process data using `.value_counts()`. The `DataFrame` containing our desired information is specified by the `data` argument, while the column to be plotted is specified by the `x` argument.\n\n---\nTo visualize the distribution of a **quantiative** variable, bar plots are no longer an appropriate choice of visualization. To see why, consider what happens when we use `sns.countplot` to visualize the distribution of gross national income, a quantitative continuous variable.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nsns.countplot(data = wb, x = 'Gross national income per capita, Atlas method: $: 2016');\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-6-output-1.png){width=615 height=429}\n:::\n:::\n\n\nWhat happened? A bar plot (either `plt.bar` or `sns.countplot`) will create a separate bar for *each* unique value of a variable. With a continuous variable, we may not have a finite number of possible values, which can lead to situations where we would need many, many bars to display each unique value. \n\nTo visualize the distribution of a continuous variable, we use a different type of plot:\n\n* Box plot\n* Violin plot\n* Histogram\n\n### Box Plots\n\n**Box plots** display distributions using information about quartiles. \n\nA quartile represents a 25% portion of the data. We say that:\n* The first quartile (Q1) repesents the 25th percentile – 25% of the data lies below the first quartile\n* The second quartile (Q2) represents the 50th percentile, also known as the median – 50% of the data lies below the second quartile\n* The third quartile (Q3) represents the 75th percentile – 75% of the data lies below the third quartile.\n\nIn a box plot, the lower extent of the box lies at Q1, while the upper extent of the box lies at Q3. The horizontal line in the middle of the box corresponds to Q2 (equivalently, the median).\n\nThe **interquartile range** measures the spread of the middle $50$% of the distribution, calculated as the ($3^{rd}$ Quartile $-$ $1^{st}$ Quartile).\n\nThe **whiskers** of a box-plot are the two points that lie at the \\[$1^{st}$ Quartile $-$ ($1.5\\times$ IQR)\\], and the \\[$3^{rd}$ Quartile $+$ ($1.5\\times$ IQR)\\]. They are the lower and upper ranges of \"normal\" data (the points excluding outliers). Subsequently, the **outliers** are the data points that fall beyond the whiskers, or further than ($1.5 \\times$ IQR) from the extreme quartiles.\n\n<img src=\"images/boxplot.png\" alt='boxplot' width='500'>\n\nIn Matplotlib, box plots are created using the function `plt.boxplot`.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# For simplicity, remove NaN values\nwb = wb[~wb[\"Gross domestic product: % growth : 2016\"].isna()]\n\nplt.boxplot(wb['Gross domestic product: % growth : 2016'])\n\nplt.ylabel(\"% GDP growth\")\nplt.title(\"Distribution of GDP growth across countries\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-7-output-1.png){width=596 height=431}\n:::\n:::\n\n\nAnd in Seaborn, box plots are generated by `sns.boxplot`.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nsns.boxplot(data = wb, y = \"Gross domestic product: % growth : 2016\")\n\nplt.title(\"Distribution of GDP growth across countries\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-8-output-1.png){width=596 height=413}\n:::\n:::\n\n\nSometimes, we may wish to also incorporate a *qualiatative* variable. For example, we may wish to compare the distribution of a quantitative variable *across* a set of categories.\n\nTo do so, we can plot **side-by-side box plots**. In the example below, the continent of each county, a qualitative nominal variable, is plotted on the x-axis, while GDP growth, a quantiative continuous variable, is plotted on the y-axis.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nsns.boxplot(data=wb, x= \"Continent\", y= \"Gross domestic product: % growth : 2016\")\n\nplt.title(\"Distribution of GDP growth by continent\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-9-output-1.png){width=596 height=449}\n:::\n:::\n\n\n### Violin Plots\n\n**Violin plots**, much like box plots, encode information about quantiles. A violin plot *also* incorporates smoothed density curves to add an additional level of information – now, the width of the plot at each point also carries meaning. \n\nThe three quartiles and whiskers that we saw in box plots are still present in violin plots (look closely at the center)!\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# In Seaborn: sns.violinplot\nsns.violinplot(data = wb, y = 'Gross domestic product: % growth : 2016')\n\nplt.title(\"Distribution of GDP growth across countries\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-10-output-1.png){width=596 height=413}\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# In Matplotlib: plt.violinplot\nplt.violinplot(wb['Gross domestic product: % growth : 2016'])\n\nplt.ylabel(\"% GDP growth\")\nplt.title(\"Distribution of GDP growth across countries\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-11-output-1.png){width=596 height=431}\n:::\n:::\n\n\n### Histograms\n\n**Histograms** are an alternative method of visualizing quantitative continuous distributions. You first encountered them in [Data 8](https://inferentialthinking.com/chapters/07/2/Visualizing_Numerical_Distributions.html#histogram). In a histogram, datapoints with similar values are collected into a shared \"bin\". These bins are scaled such that the area of each bin is equal to the percentage of datapoints it contains. \n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# In Matplotlib: plt.hist\n# The `edgecolor` argument controls the color of the bin edges\ngni = wb[\"Gross national income per capita, Atlas method: $: 2016\"]\nplt.hist(gni, density=True, edgecolor=\"white\")\n\n# Add labels\nplt.xlabel(\"Gross national income per capita\")\nplt.ylabel(\"Density\")\nplt.title(\"Distribution of gross national income per capita\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-12-output-1.png){width=576 height=449}\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# In Seaborn: sns.histplot\nsns.histplot(data = wb, x = \"Gross national income per capita, Atlas method: $: 2016\", stat=\"density\")\n\nplt.title(\"Distribution of gross national income per capita\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-13-output-1.png){width=623 height=449}\n:::\n:::\n\n\nWe can overlay histograms (or density curves) to compare distributions across qualitative categories.\n\nThe `hue` parameter of `sns.histplot` specifies the column that should be used to determine the color of each category. `hue` can be used in many Seaborn plotting functions.\n\nNotice that the resulting plot includes a legend describing which color corresponds to each hemisphere – a legend should **always** be included if color is used to encode information in a visualization! This assists with our visualization goal of communicating our results to a broader audience.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\n# Create a new variable to store the hemisphere in which each country is located\nnorth = [\"Asia\", \"Europe\", \"N. America\"]\nsouth = [\"Africa\", \"Oceania\", \"S. America\"]\nwb.loc[wb[\"Continent\"].isin(north), \"Hemisphere\"] = \"Northern\"\nwb.loc[wb[\"Continent\"].isin(south), \"Hemisphere\"] = \"Southern\"\n```\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nsns.histplot(data = wb, x = \"Gross national income per capita, Atlas method: $: 2016\", \\\n                        hue=\"Hemisphere\", stat=\"density\")\n\nplt.title(\"Distribution of gross national income per capita\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-15-output-1.png){width=576 height=449}\n:::\n:::\n\n\nYou may have seen histograms drawn differently – perhaps with an overlaid **density curve** and normalized y-axis. We can display both with a few tweaks to our code. \n\nTo visualize a density curve, we can set the the `kde = True` argument of the `sns.histplot`. Setting the argument `stat = 'density'` normalizes our histogram and displays densities, instead of counts, on the y-axis. You'll notice that the area under the density curve is 1.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nsns.histplot(data = wb, x = \"Gross national income per capita, Atlas method: $: 2016\", \\\n             kde = True, \\\n             stat = 'density');\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-16-output-1.png){width=623 height=429}\n:::\n:::\n\n\n#### Interpreting Histograms\n\nHistograms allow us to assess a distribution by its shape. In doing so, there are a few features that we typically consider.\n\nThe **skew** of a histogram describes the direction in which its \"tail\" extends. If a distribution has a long right tail (such as gross national income per capita), it is **skewed right**. In a right-skewed distribution, the few large outliers \"pull\" the mean to the right of the median.\n\nIf a distribution has a long left tail (such as water source access), it is **skewed left**. In a left-skewed distribution, the few small outliers \"pull\" the mean to the left of the median. \n\nIn the case where a distribution has equal-sized right and left tails, it is **symmetric**. The mean is approximately equal to the median. Think of mean as the balancing point of the distribution\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nimport numpy as np\n\nsns.histplot(data = wb, x = \"Gross national income per capita, Atlas method: $: 2016\", stat = \"density\");\n\ndf_mean = np.mean(wb[\"Gross national income per capita, Atlas method: $: 2016\"])\ndf_median = np.nanmedian(wb[\"Gross national income per capita, Atlas method: $: 2016\"])\n\nprint(f\"The mean, {df_mean}, is right of the median, {df_median}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe mean, 12963.312101910828, is right of the median, 5280.0\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-17-output-2.png){width=623 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nsns.histplot(data = wb, x = 'Access to an improved water source: % of population: 2015', stat = \"density\");\n\ndf_mean = np.mean(wb['Access to an improved water source: % of population: 2015'])\ndf_median = np.nanmedian(wb['Access to an improved water source: % of population: 2015'])\n\nprint(f\"The mean, {df_mean}, is left of the median, {df_median}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe mean, 88.8896103896104, is left of the median, 96.0\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-18-output-2.png){width=597 height=429}\n:::\n:::\n\n\nA **mode** of a distribution is a local or global maximum. A distribution with a single clear maximum is **unimodal**, distributions with two modes are **bimodal**, and those with 3 or more are **multimodal**. \n\nYou may need to distinguish between modes and *random noise*. Often, it is difficult to discern noise from true modes when working with histograms – a histogram may contain too much detail to be able to identify general trends across the distribution. One solution to this issue, which we will introduce next, is to construct a kernel density estimate.\n\n<img src=\"images/modes.png\" alt='modes' width='600'>\n\n### Kernel Density Estimation\n\n#### KDE Theory\n\nA **kernel density estimate (KDE)** is a smooth, continuous function that approximates a curve. They allow us to represent general trends in a distribution without focusing on the details, which is useful to analyzing the broad structure of a dataset. \n\nMore formally, a KDE attempts to approximate the underlying **probability distribution** from which our dataset was drawn. You may have encountered the idea of a probability distribution in your other classes; if not, we'll discuss it at length in Data 100 next lecture. For now, you can think of a probability distribution as a description of how likely it is for us to sample a particular value in our dataset. \n\nA KDE curve estimates the probability density function of a random variable. Consider the example below, where we have used `sns.displot` to plot both a histogram (containing the datapoints we actually collected) and a KDE curve (representing the *approximated* probability distribution from which this data was drawn). \n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nsns.displot(data = wb, x = 'Antiretroviral therapy coverage: % of people living with HIV: 2015', \\\n                       kde = True, stat = \"density\")\n\nplt.title(\"Distribution of HIV rates\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-19-output-1.png){width=470 height=490}\n:::\n:::\n\n\nNotice that the smooth KDE curve is higher when the histogram bins are taller. You can think of the height of the KDE curve as representing how \"probable\" it is that we randomly sample a datapoint with the corresponding value. This intuitively makes sense – if we have already collected more datapoints with a particular value (resulting in a tall histogram bin), it is more likely that, if we randomly sample another datapoint, we will sample one with a similar value (resulting in a high KDE curve).\n\nThe area under a probability density function should always integrate to 1, representing the fact that the total probability of a distribution should always sum to 100%. Because of this, a KDE curve will always have an area of 1 under the curve.\n\n#### Constructing a KDE\n\nWe perform kernel density estimation using three steps.\n\n1. Place a kernel at each data point\n2. Normalize kernels to have total area of 1 (across all kernels)\n3. Sum kernels together\n\nWe'll explain what a \"kernel\" is momentarily.\n\nTo make things simpler, let's construct a KDE for a small, artificially-generated dataset of 5 datapoints: $[2.2, 2.8, 3.7, 5.3, 5.7]$. In the plot below, each vertical bar represents one datapoint.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\"}\ndata = [2.2, 2.8, 3.7, 5.3, 5.7]\n\nsns.rugplot(data, height=0.3)\n\nplt.xlabel(\"Data\")\nplt.ylabel(\"Density\")\nplt.xlim(-3, 10)\nplt.ylim(0, 0.5);\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-20-output-1.png){width=597 height=434}\n:::\n:::\n\n\nOur goal is to create the following KDE curve, which was generated automatically by `sns.kdeplot`.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\"}\nsns.kdeplot(data)\n\nplt.xlabel(\"Data\")\nplt.xlim(-3, 10)\nplt.ylim(0, 0.5);\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-21-output-1.png){width=597 height=434}\n:::\n:::\n\n\n##### Step 1: Place a Kernel at Each Datapoint\n\nTo begin generating a density curve, we need to choose a **kernel** and **bandwidth value ($\\alpha$)**. What are these exactly? \n\nA **kernel** is a density curve. It is the mathematical function that attempts to capture the randomness of each datapoint in our sampled data. To explain what this means, consider just *one* of the datapoints in our dataset: $2.2$. We obtained this datapoint by randomly sampling some information out in the real world (you can imagine $2.2$ as representing a single measurement taken in an experiment, for example). If we were to sample a new datapoint, we may obtain a slightly different value. It could be higher than $2.2$; it could also be lower than $2.2$. We make the assumption that any future sampled datapoints will likely be similar in value to the data we've already drawn. This means that our *kernel* – our description of the probability of randomly sampling any new value – will be greatest at the datapoint we've already drawn, but still have non-zero probability above and below it. The area under any kernel should integrate to 1, representing the total probability of drawing a new datapoint.\n\nA **bandwidth value**, usually denoted by $\\alpha$, represents the width of the kernel. A large value of $\\alpha$ will result in a wide, short kernel function, while a small value with result in a narrow, tall kernel.\n\nBelow, we place a **Gaussian kernel**, plotted in orange, over the datapoint $2.2$. A Gaussian kernel is simply the normal distribution, which you may have called a bell curve in Data 8. \n\n::: {.cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\"}\ndef gaussian_kernel(x, z, a):\n    # We'll discuss where this mathematical formulation came from later\n    return (1/np.sqrt(2*np.pi*a**2)) * np.exp((-(x - z)**2 / (2 * a**2)))\n\n# Plot our datapoint\nsns.rugplot([2.2], height=0.3)\n\n# Plot the kernel\nx = np.linspace(-3, 10, 1000)\nplt.plot(x, gaussian_kernel(x, 2.2, 1))\n\nplt.xlabel(\"Data\")\nplt.ylabel(\"Density\")\nplt.xlim(-3, 10)\nplt.ylim(0, 0.5);\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-22-output-1.png){width=597 height=434}\n:::\n:::\n\n\nTo begin creating our KDE, we place a kernel on *each* datapoint in our dataset. For our dataset of 5 points, we will have 5 kernels.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\"}\n# You will work with the functions below in Lab 4\ndef create_kde(kernel, pts, a):\n    # Takes in a kernel, set of points, and alpha\n    # Returns the KDE as a function\n    def f(x):\n        output = 0\n        for pt in pts:\n            output += kernel(x, pt, a)\n        return output / len(pts) # Normalization factor\n    return f\n\ndef plot_kde(kernel, pts, a):\n    # Calls create_kde and plots the corresponding KDE\n    f = create_kde(kernel, pts, a)\n    x = np.linspace(min(pts) - 5, max(pts) + 5, 1000)\n    y = [f(xi) for xi in x]\n    plt.plot(x, y);\n    \ndef plot_separate_kernels(kernel, pts, a, norm=False):\n    # Plots individual kernels, which are then summed to create the KDE\n    x = np.linspace(min(pts) - 5, max(pts) + 5, 1000)\n    for pt in pts:\n        y = kernel(x, pt, a)\n        if norm:\n            y /= len(pts)\n        plt.plot(x, y)\n    \n    plt.show();\n    \nplt.xlim(-3, 10)\nplt.ylim(0, 0.5)\nplt.xlabel(\"Data\")\nplt.ylabel(\"Density\")\n\nplot_separate_kernels(gaussian_kernel, data, a = 1)\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-23-output-1.png){width=597 height=434}\n:::\n:::\n\n\n##### Step 2: Normalize Kernels to Have Total Area of 1\n\nAbove, we said that *each* kernel has an area of 1. Earlier, we also said that our goal is to construct a KDE curve using these kernels with a *total* area of 1. If we were to directly sum the kernels as they are, we would produce a KDE curve with an integrated area of (5 kernels) $\\times$ (area of 1 each) = 5. To avoid this, we will **normalize** each of our kernels. This involves multiplying each kernel by $1/(\\#\\:\\text{datapoints})$. \n\nIn the cell below, we multiply each of our 5 kernels by $\\frac{1}{5}$ to apply normalization.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code code-fold=\"true\"}\nplt.xlim(-3, 10)\nplt.ylim(0, 0.5)\nplt.xlabel(\"Data\")\nplt.ylabel(\"Density\")\n\n# The `norm` argument specifies whether or not to normalize the kernels\nplot_separate_kernels(gaussian_kernel, data, a = 1, norm = True)\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-24-output-1.png){width=597 height=434}\n:::\n:::\n\n\n##### Step 3: Sum Kernels Together\n\nOur KDE curve is the sum of the normalized kernels. Notice that the final curve is identical to the plot generated by `sns.kdeplot` we saw earlier!\n\n::: {.cell execution_count=24}\n``` {.python .cell-code code-fold=\"true\"}\nplt.xlim(-3, 10)\nplt.ylim(0, 0.5)\nplt.xlabel(\"Data\")\nplt.ylabel(\"Density\")\n\nplot_kde(gaussian_kernel, data, a = 1)\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-25-output-1.png){width=597 height=434}\n:::\n:::\n\n\n#### Kernel Functions and Bandwidths\n\nA kernel, for our purposes, is any valid probability density function. This means that a kernel function must:\n\n* Be non-negative for all values (we can't have negative probability)\n* Integrate to 1\n\n##### Gaussian Kernel\n\nAs we saw above, a Gaussian kernel (also called the normal distribution or bell curve) is one of the most common choices of kernel functions. It is defined mathematically as:\n\n$$K_a(x, x_i) = \\frac{1}{\\sqrt{2\\pi\\alpha^{2}}}e^{-\\frac{(x-x_i)^{2}}{2\\alpha^{2}}}$$\n\nIn this formula:\n\n* $x$ (no subscript) represents values along the x-axis of our plot\n* $x_i$ represents the $i$th datapoint in our dataset. It is one of the values that we have actually collected in our data sampling process. In our example earlier, $x_i=2.2$. Those of you who have taken a probability class may recognize $x_i$ as the **mean** of the normal distribution.\n* $\\alpha$ is the bandwidth parameter, representing the width of our kernel. More formally, $\\alpha$ is the **standard deviation** of the Gaussian curve.\n\nThe details of this (admittedly intimidating) formula are less important than understanding its role in kernel density estimation – this equation gives us the shape of each kernel.\n\nA large value of $\\alpha$ will produce a kernel that is wider and shorter – this leads to a smoother KDE when the kernels are summed together. Conversely, a small value of $\\alpha$ will produce a narrower, taller kernel, and, with it, a noisier KDE.\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n**Gaussian Kernel, $\\alpha$ = 0.1**\n\n<img src=\"images/gaussian_0.1.png\" alt='gaussian_0.1' width='345'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Gaussian Kernel, $\\alpha$ = 1**\n\n<img src=\"images/gaussian_1.png\" alt='gaussian_1' width='345'>\n:::\n\n::::\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n**Gaussian Kernel, $\\alpha$ = 2**\n\n<img src=\"images/gaussian_2.png\" alt='gaussian_2' width='345'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Gaussian Kernel, $\\alpha$ = 10**\n\n<img src=\"images/gaussian_10.png\" alt='gaussian_10' width='345'>\n:::\n\n::::\n\n#### Boxcar Kernel\n\nAnother example of a kernel is the **Boxcar kernel**. The boxcar kernel assigns a uniform density to points within a \"window\" of the observation, and a density of 0 elsewhere. The equation below is a Boxcar kernel with the center at $x_i$ and the bandwidth of $\\alpha$.\n\n$$K_a(x, x_i) = \\begin{cases}\n        \\frac{1}{\\alpha}, & |x - x_i| \\le \\frac{\\alpha}{2}\\\\\n        0, & \\text{else }\n    \\end{cases}$$\n    \nThe boxcar kernel is seldom used in practice – we include it here to demonstrate that a kernel function can take whatever form you would like, provided it integrates to 1 and does not output negative values. In the cell below, we plot the boxcar kernel at our datapoint from before, $2.2$.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code code-fold=\"true\"}\ndef boxcar_kernel(x, z, alpha):\n    return (((x-z)>=-alpha/2)&((x-z)<=alpha/2))/alpha\n\n# Plot our datapoint\nsns.rugplot([2.2], height=0.3)\n\nplt.xlabel(\"Data\")\nplt.ylabel(\"Density\")\nplt.xlim(-3, 10)\nplt.ylim(0, 0.5)\n\n# Plot the kernel\nx = np.linspace(-3, 10, 1000)\nplot_separate_kernels(boxcar_kernel, [2.2], a = 2.5)\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-26-output-1.png){width=597 height=434}\n:::\n:::\n\n\nUsing the boxcar kernel with $\\alpha=2.5$ to generate a KDE for our full dataset yields the following result:\n\n::: {.cell execution_count=26}\n``` {.python .cell-code code-fold=\"true\"}\nplt.xlabel(\"Data\")\nplt.ylabel(\"Density\")\nplt.xlim(-3, 10)\nplt.ylim(0, 0.5)\n\n# Plot the kernel\nx = np.linspace(-3, 10, 1000)\nplot_kde(boxcar_kernel, data, a = 2.5)\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-27-output-1.png){width=597 height=434}\n:::\n:::\n\n\n## Visualizing Relationships\n\nUp until now, we've discussed how to visualize single-variable distributions. Now, let's understand how to visualize the relationship between pairs of numerical variables.\n\n#### Scatter Plots\n\n**Scatter plots** are one of the most useful tools in representing the relationship between two quantitative variables. They are particularly important in gauging the strength, or correlation, of the relationship between variables. Knowledge of these relationships can then motivate decisions in our modeling process.\n\nIn Matplotlib, we use the function `plt.scatter` to generate a scatter plot. Notice that unlike our examples of plotting single-variable distributions, now we specify sequences of values to be plotted along the x axis *and* the y axis. \n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nplt.scatter(wb[\"per capita: % growth: 2016\"], \\\n            wb['Adult literacy rate: Female: % ages 15 and older: 2005-14'])\n\nplt.xlabel(\"% growth per capita\")\nplt.ylabel(\"Female adult literacy rate\")\nplt.title(\"Female adult literacy against % growth\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-28-output-1.png){width=593 height=449}\n:::\n:::\n\n\nIn Seaborn, we call the function `sns.scatterplot`. We use the `x` and `y` parameters to indicate the values to be plotted along the x and y axes, respectively. By using the `hue` parameter, we can specify a third variable to be used for coloring each scatter point.\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nsns.scatterplot(data = wb, x = \"per capita: % growth: 2016\", \\\n               y = \"Adult literacy rate: Female: % ages 15 and older: 2005-14\", \n               hue = \"Continent\")\n\nplt.title(\"Female adult literacy against % growth\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-29-output-1.png){width=593 height=449}\n:::\n:::\n\n\nAlthough the plots above communicate the general relationship between the two plotted variables, they both suffer a major limitation – **overplotting**. Overplotting occurs when scatter points with similar values are stacked on top of one another, making it difficult to see the number of scatter points actually plotted in the visualization. Notice how in the upper righthand region of the plots, we cannot easily tell just how many points have been plotted. This make our visualizations difficult to interpret. \n\nWe have a few methods to help reduce overplotting:\n\n* Decreasing the size of the scatter point markers can improve readability. We do this by setting a new value to the size parameter, `s`, of `plt.scatter` or `sns.scatterplot`\n* **Jittering** is the process of adding a small amount of random noise to all x and y values to slightly shift the position of each datapoint. By randomly shifting all the data by some small distance, we can discern individual points more clearly without modifying the major trends of the original dataset.\n\nIn the cell below, we first jitter the data using `np.random.uniform`, then re-plot it with smaller markers. The resulting plot is much easier to interpret.\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\n# Setting a seed ensures that we produce the same plot each time\n# This means that the course notes will not change each time you access them\nnp.random.seed(150)\n\n# This call to np.random.uniform generates random numbers between -1 and 1\n# We add these random numbers to the original x data to jitter it slightly\nx_noise = np.random.uniform(-1, 1, len(wb))\njittered_x = wb[\"per capita: % growth: 2016\"] + x_noise\n\n# Repeat for y data\ny_noise = np.random.uniform(-5, 5, len(wb))\njittered_y = wb[\"Adult literacy rate: Female: % ages 15 and older: 2005-14\"] + y_noise\n\n# Setting the size parameter `s` changes the size of each point\nplt.scatter(jittered_x, jittered_y, s=15)\n\nplt.xlabel(\"% growth per capita (jittered)\")\nplt.ylabel(\"Female adult literacy rate (jittered)\")\nplt.title(\"Female adult literacy against % growth\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-30-output-1.png){width=593 height=449}\n:::\n:::\n\n\n#### `lmplot` and `jointplot`\n\nSeaborn also includes several built-in functions for creating more sophisticated scatter plots. Two of the most commonly-used examples are `sns.lmplot` and `sns.jointplot`. \n\n`sns.lmplot` plots both a scatter plot *and* a linear regression line, all in one function call. We'll discuss linear regression in a few lectures. \n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nsns.lmplot(data = wb, x = \"per capita: % growth: 2016\", \\\n           y = \"Adult literacy rate: Female: % ages 15 and older: 2005-14\")\n\nplt.title(\"Female adult literacy against % growth\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-31-output-1.png){width=470 height=490}\n:::\n:::\n\n\n`sns.jointplot` creates a visualization with three components: a scatter plot, a histogram of the distribution of x values, and a histogram of the distribution of y values.\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nsns.jointplot(data = wb, x = \"per capita: % growth: 2016\", \\\n           y = \"Adult literacy rate: Female: % ages 15 and older: 2005-14\")\n\n# plt.suptitle allows us to shift the title up so it does not overlap with the histogram\nplt.suptitle(\"Female adult literacy against % growth\")\nplt.subplots_adjust(top=0.9);\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-32-output-1.png){width=570 height=569}\n:::\n:::\n\n\n#### Hex plots\nFor datasets with a very large number of datapoints, jittering is unlikely to fully resolve the issue of overplotting. In these cases, we can attempt to visualize our data by its *density*, rather than displaying each individual datapoint.\n\n**Hex plots** can be thought of as a two dimensional histograms that shows the joint distribution between two variables. This is particularly useful working with very dense data. In a hex plot, the x-y plane is binned into hexagons. Hexagons that are darker in color indicate a greater density of data – that is, there are more datapoints that lie in the region enclosed by the hexagon.\n\nWe can generate a hex plot using `sns.jointplot` modified with the `kind` parameter.\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nsns.jointplot(data = wb, x = \"per capita: % growth: 2016\", \\\n              y = \"Adult literacy rate: Female: % ages 15 and older: 2005-14\", \\\n              kind = \"hex\")\n\n# plt.suptitle allows us to shift the title up so it does not overlap with the histogram\nplt.suptitle(\"Female adult literacy against % growth\")\nplt.subplots_adjust(top=0.9);\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-33-output-1.png){width=570 height=569}\n:::\n:::\n\n\n#### Contour Plots\n\n**Contour plots** are an alternative way of plotting the joint distribution of two variables. You can think of them as the 2-dimensional versions of KDE plots. A contour plot can be interpreted in a similar way to a [topographic map](https://gisgeography.com/contour-lines-topographic-map/). Each countour line represents an area that has the same *density* of datapoints throughout the region. Contours marked with darker colors contain more datapoints (a higher density) in that region.\n\n`sns.kdeplot` will generate a contour plot if we specify both x and y data.\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nsns.kdeplot(data = wb, x = \"per capita: % growth: 2016\", \\\n            y = \"Adult literacy rate: Female: % ages 15 and older: 2005-14\", \\\n            fill = True)\n\nplt.title(\"Female adult literacy against % growth\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-34-output-1.png){width=596 height=449}\n:::\n:::\n\n\n## Transformations\n\nThis lecture has covered visualizations in great depth. We looked at various forms of visualizations, plotting libraries, and high-level theory.\n\nMuch of this was done to uncover insights in data, which will prove necessary when we begin building models of data later in the course. A strong graphical correlation between two variables hints at an underlying relationship that we may want to study in greater detail. However, relying on visual relationships alone is limiting - not all plots show association. The presence of outliers and other statistical anomalies make it hard to interpret data.\n\n**Transformations** are the process of manipulating data to find significant relationships between variables. These are often found by applying mathematical functions to variables that \"transform\" their range of possible values and highlight some previously hidden associations between data.\n\nTo see why we may want to transform data, consider the following plot of adult literacy rates against gross national income.\n\n::: {.cell execution_count=34}\n``` {.python .cell-code code-fold=\"true\"}\n# Some data cleaning to help with the next example\ndf = pd.DataFrame(index=wb.index)\ndf['lit'] = wb['Adult literacy rate: Female: % ages 15 and older: 2005-14'] \\\n            + wb[\"Adult literacy rate: Male: % ages 15 and older: 2005-14\"]\ndf['inc'] = wb['Gross national income per capita, Atlas method: $: 2016']\ndf.dropna(inplace=True)\n\nplt.scatter(df[\"inc\"], df[\"lit\"])\nplt.xlabel(\"Gross national income per capita\")\nplt.ylabel(\"Adult literacy rate\")\nplt.title(\"Adult literacy rate against GNI per capita\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-35-output-1.png){width=593 height=449}\n:::\n:::\n\n\nThis plot is difficult to interpret for two reasons:\n\n* The data shown in the visualization appears almost \"smushed\" – it is heavily concentrated in the upper lefthand region of the plot. Even if we jittered the dataset, we likely would not be able to fully assess all datapoints in that area.\n* It is hard to generalize a clear relationship between the two plotted variables. While adult literacy rate appears to share some positive relationship with gross national income, we are not able to describe the specifics in this trend in much detail.\n\nA transformation would allow us to visualize this data more clearly, which, in turn, would enable us to describe the underlying relationship between our variables of interest.\n\nWe will most commonly apply a transformation to **linearize a relationship** between variables. If we find a transformation to make a scatter plot of two variables linear, we can \"backtrack\" to find the exact relationship between the variables. This helps us in two major ways. Firstly, linear relationships are particularly simple to interpret – we have an intutive sense of what the slope and intercept of a linear trend represent, and how they can help us understand the relationship between two variables. Secondly, linear relationships are the backbone of linear models. We will begine exploring linear modeling in great detail next week. As we'll soon see, linear models become much more effective when we are working with linearized data.  \n\nIn the remainder of this note, we will discuss how to linearize a dataset to produce the result below. Notice that the resulting plot displays a rough linear relationship between the values plotted on the x and y axes.\n\n<img src=\"images/linearize.png\" alt='linearize' width='700'>\n\n### Applying Transformations\n\nTo linearize a relationship, begin by asking yourself: what makes the data non-linear? It is helpful to repeat this question for each variable in your visualization.\n\nLet's start by considering the gross national income variable in our plot above. Looking at *just* the x values in the scatter plot, we notice that most datapoints are clustered at low values to the left of the plot. The scale of the x axis is being distorted by the few outlying datapoints at extremely large values on the right. \n\n<img src=\"images/horizontal.png\" alt='horizontal' width='450'>\n\nIf we decreased the size of these outliers relative to the bulk of the data, we could reduce the distortion of the horizontal axis. How can we do this? We need a transformation that will:\n\n* Decrease the magnitude of large x values by a signficant amount\n* Not drastically change the magnitude of small x values\n\nOne function that produces this result is the **log transformation**. When we take the logarithm of a large number, the original number will decrease in magnitude dramatically. Conversely, when we take the logarithm of a small number, the original number does not change its value by as significant of an amount (to illustrate this, consider the difference between $\\log{(100)} = 4.61$ and $\\log{(10)} = 2.3$).\n\nIn Data 100 (and most upper division STEM classes), $\\log$ is used to refer to the natural logarithm with base $e$.\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\n# np.log takes the logarithm of an array or Series\nplt.scatter(np.log(df[\"inc\"]), df[\"lit\"])\n\nplt.xlabel(\"Log(gross national income per capita)\")\nplt.ylabel(\"Adult literacy rate\")\nplt.title(\"Adult literacy rate against Log(GNI per capita)\");\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-36-output-1.png){width=593 height=449}\n:::\n:::\n\n\nAfter taking the logarithm of our x values, our plot appears much more balanced in its horizontal scale. We no longer have many datapoints clumped on one end and a few outliers out at extreme values. \n\nLet's repeat this reasoning for the y values. Considering only the vertical axis of the plot, notice how there are many datapoints concentrated at large y values. Only a few datapoints lie at smaller values of y.\n\n<img src=\"images/vertical.png\" alt='vertical' width='550'>\n\nIf we were to \"spread out\" these large values of y more, we would no longer see the dense concentration in one region of the y axis. We need a transformation that will:\n\n* Increase the magnitude of large values of y so these datapoints are distributed more broadly on the vertical scale\n* Not substantially alter the scaling of small values of y (we do not want to drastically modify the lower end of the y axis, which is already distributed evenly on the vertical scale)\n\nIn this case, it is helpful to apply a **power transformation** – that is, raise our y values to a power. Let's try raising our adult literacy rate values to the power of 4. Large values raised to the power of 4 will increase in magnitude proportionally much more than small values raised to the power of 4 (consider the difference between $2^4 = 16$ and $200^4 = 1600000000$). \n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\n# Apply a log transformation to the x values and a power transformation to the y values\nplt.scatter(np.log(df[\"inc\"]), df[\"lit\"]**4)\n\nplt.xlabel(\"Log(gross national income per capita)\")\nplt.ylabel(\"Adult literacy rate (4th power)\")\nplt.suptitle(\"Adult literacy rate (4th power) against Log(GNI per capita)\")\nplt.subplots_adjust(top=0.9);\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-37-output-1.png){width=589 height=477}\n:::\n:::\n\n\nOur scatter plot is looking a lot better! Now, we are plotting the log of our original x values on the horizontal axis, and the 4th power of our original y values on the vertical axis. We start to see an approximate *linear* relationship between our transformed variables. \n\nWhat can we take away from this? We now know that the log of gross national income and adult literacy to the power of 4 are roughly linearly related. If we denote the original, untransformed gross national income values as $x$ and the original adult literacy rate values as $y$, we can use the standard form of a linear fit to express this relationship:\n\n$$y^4 = m(\\log{x}) + b$$\n\nWhere $m$ represents the slope of the linear fit, while $b$ represents the intercept. \n\nThe cell below computes $m$ and $b$ for our transformed data. We'll discuss how this code was generated in a future lecture.\n\n::: {.cell execution_count=37}\n``` {.python .cell-code code-fold=\"true\"}\n# The code below fits a linear regression model. We'll discuss it at length in a future lecture\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(np.log(df[[\"inc\"]]), df[\"lit\"]**4)\nm, b = model.coef_[0], model.intercept_\n\nprint(f\"The slope, m, of the transformed data is: {m}\")\nprint(f\"The intercept, b, of the transformed data is: {b}\")\n\ndf = df.sort_values(\"inc\")\nplt.scatter(np.log(df[\"inc\"]), df[\"lit\"]**4, label=\"Transformed data\")\nplt.plot(np.log(df[\"inc\"]), m*np.log(df[\"inc\"])+b, c=\"red\", label=\"Linear regression\")\nplt.xlabel(\"Log(gross national income per capita)\")\nplt.ylabel(\"Adult literacy rate (4th power)\")\nplt.legend();\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe slope, m, of the transformed data is: 336400693.43172705\nThe intercept, b, of the transformed data is: -1802204836.0479987\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-38-output-2.png){width=597 height=443}\n:::\n:::\n\n\nWhat if we want to understand the *underlying* relationship between our original variables, before they were transformed? We can simply rearrange our linear expression above! \n\nRecall our linear relationship between the transformed variables $\\log{x}$ and $y^4$. \n\n$$y^4 = m(\\log{x}) + b$$\n\nBy rearranging the equation, we find a relationship between the untransformed variables $x$ and $y$. \n\n$$y = [m(\\log{x}) + b]^{(1/4)}$$\n\nWhen we plug in the values for $m$ and $b$ computed above, something interesting happens.\n\n::: {.cell execution_count=38}\n``` {.python .cell-code code-fold=\"true\"}\n# Now, plug the values for m and b into the relationship between the untransformed x and y\nplt.scatter(df[\"inc\"], df[\"lit\"], label=\"Untransformed data\")\nplt.plot(df[\"inc\"], (m*np.log(df[\"inc\"])+b)**(1/4), c=\"red\", label=\"Modeled relationship\")\nplt.xlabel(\"Gross national income per capita\")\nplt.ylabel(\"Adult literacy rate\")\nplt.legend();\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_1_files/figure-html/cell-39-output-1.png){width=593 height=429}\n:::\n:::\n\n\nWe have found a relationship between our original variables – gross national income and adult literacy rate!\n\nTransformations are powerful tools for understanding our data in greater detail. To summarize what we just achieved:\n\n* We identified appropriate transformations to **linearize** the original data\n* We used our knowledge of linear curves to compute the slope and intercept of the transformed data\n* We used this slope and intercept information to derive a relationship in the untransformed data\n\nLinearization will be an important tool as we begin our work on linear modeling next week.\n\n### Tukey-Mosteller Bulge Diagram\n\nThe **Tukey-Mosteller Bulge Diagram** is a guide to possible transformations for achieving linearity. It is a visual summary of the reasoning we just worked through above. \n\n<img src=\"images/tukey_mosteller.png\" alt='tukey_mosteller' width='300'>\n\nHow does it work? Each curved \"bulge\" represents a possible shape of non-linear data. To use the diagram, find which of the four bulges resembles your dataset the most closely. Then, look at the axes of the quadrant for this bulge. The horizontal axis will list possible transformations that could be applied to your x data for linearization. Similarly, the verical axis will list possible transformations that could be applied to your y data. Note that each axis lists two possible transformations – *either* of these transformations has the potential to linearize your dataset. It's a good idea to try both out to see which one produces better results on your specific data.\n\nGenerally:\n\n* $\\sqrt{}$ and $\\log{}$ will reduce the magnitude of large values\n* Powers ($^2$ and $^3$) will increase the spread in magnitude of large values\n\n<img src=\"images/bulge.png\" alt='bulge' width='800'>\n\n**Important:** you should still understand the *logic* we worked through to determine how best to transform the data. The bulge diagram is just a summary of this same reasoning. You will be expected to be able to explain why a given transformation is or is not appropriate for linearization.\n\n",
    "supporting": [
      "visualization_1_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}